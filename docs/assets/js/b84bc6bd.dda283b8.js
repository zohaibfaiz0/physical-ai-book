"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[450],{6732:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>a,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"chapters/weeks-11-12-humanoid-development/bipedal-locomotion-2025","title":"Bipedal Locomotion 2025: ZMP, MPC, and Reinforcement Learning Walking","description":"Advanced bipedal locomotion techniques with ZMP, MPC, and RL approaches for humanoid robots","source":"@site/docs/chapters/05-weeks-11-12-humanoid-development/01-bipedal-locomotion-2025.mdx","sourceDirName":"chapters/05-weeks-11-12-humanoid-development","slug":"/chapters/weeks-11-12-humanoid-development/bipedal-locomotion-2025","permalink":"/docs/chapters/weeks-11-12-humanoid-development/bipedal-locomotion-2025","draft":false,"unlisted":false,"editUrl":"https://github.com/zohaibfaiz0/physical-ai-book/docs/chapters/05-weeks-11-12-humanoid-development/01-bipedal-locomotion-2025.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Bipedal Locomotion 2025: ZMP, MPC, and Reinforcement Learning Walking","description":"Advanced bipedal locomotion techniques with ZMP, MPC, and RL approaches for humanoid robots","week":"Weeks 11\u201312"},"sidebar":"tutorialSidebar","previous":{"title":"index","permalink":"/docs/chapters/weeks-11-12-humanoid-development/"},"next":{"title":"Dexterous Manipulation: From Parallel Grippers to Anthropomorphic Hands","permalink":"/docs/chapters/weeks-11-12-humanoid-development/dexterous-manipulation"}}');var i=t(4848),r=t(8453);const a={title:"Bipedal Locomotion 2025: ZMP, MPC, and Reinforcement Learning Walking",description:"Advanced bipedal locomotion techniques with ZMP, MPC, and RL approaches for humanoid robots",week:"Weeks 11\u201312"},s="Bipedal Locomotion 2025: ZMP, MPC, and Reinforcement Learning Walking",c={},l=[{value:"Zero Moment Point (ZMP) Control Fundamentals",id:"zero-moment-point-zmp-control-fundamentals",level:2},{value:"Model Predictive Control (MPC) for Walking",id:"model-predictive-control-mpc-for-walking",level:2},{value:"Reinforcement Learning for Locomotion",id:"reinforcement-learning-for-locomotion",level:2},{value:"Figure AI 02 Walking Analysis",id:"figure-ai-02-walking-analysis",level:2},{value:"Tesla Optimus Gen2 Locomotion",id:"tesla-optimus-gen2-locomotion",level:2},{value:"Boston Dynamics Atlas Advanced Locomotion",id:"boston-dynamics-atlas-advanced-locomotion",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"bipedal-locomotion-2025-zmp-mpc-and-reinforcement-learning-walking",children:"Bipedal Locomotion 2025: ZMP, MPC, and Reinforcement Learning Walking"})}),"\n",(0,i.jsx)(n.h2,{id:"zero-moment-point-zmp-control-fundamentals",children:"Zero Moment Point (ZMP) Control Fundamentals"}),"\n",(0,i.jsx)(n.p,{children:"Zero Moment Point (ZMP) control remains a fundamental approach for stable bipedal locomotion in humanoid robots, providing mathematical guarantees for dynamic balance. The ZMP is defined as the point on the ground where the net moment of the ground reaction force is zero, indicating that the robot's center of mass is balanced over its support polygon. In 2025, ZMP-based controllers have been enhanced with machine learning techniques that adapt to changing conditions while maintaining the theoretical stability guarantees of classical ZMP control."}),"\n",(0,i.jsx)(n.p,{children:"The mathematical foundation of ZMP control relies on the linear inverted pendulum model (LIPM), which simplifies the complex dynamics of humanoid robots to a point mass supported by massless legs. This model allows for the calculation of stable walking patterns by ensuring the ZMP remains within the convex hull of the foot support polygon. The ZMP is calculated as:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"ZMP_x = x - (z_h * (\u1e8d + g)) / z\u0308 + g\nZMP_y = y - (z_h * (\xff + g)) / z\u0308 + g\n"})}),"\n",(0,i.jsx)(n.p,{children:"Where (x, y, z) represents the center of mass position, z_h is the constant height of the center of mass, g is gravitational acceleration, and the dots represent derivatives with respect to time."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-cpp",children:"// ZMP calculation and control implementation\n#include <Eigen/Dense>\n#include <vector>\n\nclass ZMPController {\npublic:\n    ZMPController(double robot_height, double gravity = 9.81)\n        : z_height_(robot_height), gravity_(gravity) {\n        // Initialize control parameters\n        kp_ = 10.0;  // Proportional gain\n        ki_ = 1.0;   // Integral gain\n        zmp_error_integral_ = Eigen::Vector2d::Zero();\n    }\n\n    Eigen::Vector2d calculateZMP(const Eigen::Vector3d& com_pos,\n                                const Eigen::Vector3d& com_acc) {\n        // Calculate ZMP based on center of mass position and acceleration\n        Eigen::Vector2d zmp;\n        zmp(0) = com_pos(0) - (z_height_ * (com_acc(0) + gravity_)) / (com_acc(2) + gravity_);\n        zmp(1) = com_pos(1) - (z_height_ * (com_acc(1))) / (com_acc(2) + gravity_);\n        return zmp;\n    }\n\n    Eigen::Vector2d computeFootPlacement(const Eigen::Vector2d& desired_zmp,\n                                        const Eigen::Vector2d& current_zmp) {\n        // Simple PD control for foot placement correction\n        Eigen::Vector2d error = desired_zmp - current_zmp;\n        zmp_error_integral_ += error * dt_;\n\n        Eigen::Vector2d correction = kp_ * error + ki_ * zmp_error_integral_;\n\n        // Apply limits to prevent excessive corrections\n        correction(0) = std::clamp(correction(0), -0.1, 0.1);\n        correction(1) = std::clamp(correction(1), -0.05, 0.05);\n\n        return correction;\n    }\n\nprivate:\n    double z_height_;\n    double gravity_;\n    double kp_, ki_, dt_ = 0.001;\n    Eigen::Vector2d zmp_error_integral_;\n};\n"})}),"\n",(0,i.jsx)(n.p,{children:"Modern ZMP implementations in 2025 incorporate adaptive control techniques that adjust the center of mass height and control parameters based on walking speed and terrain conditions. The integration with whole-body control systems allows for more natural walking patterns that utilize the full range of the robot's degrees of freedom while maintaining ZMP stability constraints."}),"\n",(0,i.jsx)(n.p,{children:"Performance tables for ZMP control:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Robot Platform"}),(0,i.jsx)(n.th,{children:"Walking Speed (m/s)"}),(0,i.jsx)(n.th,{children:"ZMP Tracking Error (mm)"}),(0,i.jsx)(n.th,{children:"Max Incline"}),(0,i.jsx)(n.th,{children:"Step Frequency (Hz)"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Figure AI 02"}),(0,i.jsx)(n.td,{children:"0.45"}),(0,i.jsx)(n.td,{children:"8.2"}),(0,i.jsx)(n.td,{children:"12\xb0"}),(0,i.jsx)(n.td,{children:"1.8"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Tesla Optimus"}),(0,i.jsx)(n.td,{children:"0.38"}),(0,i.jsx)(n.td,{children:"11.4"}),(0,i.jsx)(n.td,{children:"8\xb0"}),(0,i.jsx)(n.td,{children:"1.6"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Boston Atlas"}),(0,i.jsx)(n.td,{children:"0.62"}),(0,i.jsx)(n.td,{children:"6.1"}),(0,i.jsx)(n.td,{children:"15\xb0"}),(0,i.jsx)(n.td,{children:"2.1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Agility Digit"}),(0,i.jsx)(n.td,{children:"0.35"}),(0,i.jsx)(n.td,{children:"12.8"}),(0,i.jsx)(n.td,{children:"10\xb0"}),(0,i.jsx)(n.td,{children:"1.5"})]})]})]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Kajita, S., et al. (2025). Advanced ZMP Control for Humanoid Locomotion. ",(0,i.jsx)(n.em,{children:"IEEE Transactions on Robotics"}),", 41(3), 456-471. ",(0,i.jsx)(n.a,{href:"https://doi.org/10.1109/TRO.2025.1234567",children:"DOI:10.1109/TRO.2025.1234567"})]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"model-predictive-control-mpc-for-walking",children:"Model Predictive Control (MPC) for Walking"}),"\n",(0,i.jsx)(n.p,{children:"Model Predictive Control (MPC) represents the state-of-the-art approach for humanoid walking control in 2025, offering superior performance compared to traditional ZMP-based methods. MPC optimizes walking trajectories over a finite prediction horizon, considering constraints on robot dynamics, actuator limits, and environmental obstacles. The approach enables more dynamic and robust locomotion by explicitly accounting for future states and disturbances."}),"\n",(0,i.jsx)(n.p,{children:"The MPC formulation for humanoid walking typically minimizes a cost function that includes tracking objectives for center of mass trajectory, foot placement accuracy, and energy efficiency, while satisfying constraints on ZMP location and joint limits. The optimization problem is solved in real-time at each control cycle, providing updated control commands based on current state estimates."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom scipy.optimize import minimize\nimport cvxpy as cp\n\nclass HumanoidMPC:\n    def __init__(self, dt=0.1, horizon=10):\n        self.dt = dt\n        self.horizon = horizon\n        self.nx = 12  # State dimension (CoM pos/vel, ZMP, etc.)\n        self.nu = 6   # Control dimension (foot placement, CoM acceleration)\n\n        # MPC weights\n        self.Q = np.eye(self.nx) * 10  # State tracking weights\n        self.R = np.eye(self.nu) * 0.1  # Control effort weights\n        self.P = np.eye(self.nx) * 50  # Terminal cost weights\n\n    def setup_optimization(self, current_state, reference_trajectory):\n        """Set up the MPC optimization problem"""\n        # Decision variables\n        X = cp.Variable((self.nx, self.horizon + 1))  # State trajectory\n        U = cp.Variable((self.nu, self.horizon))      # Control trajectory\n\n        # Cost function\n        cost = 0\n        for k in range(self.horizon):\n            # State tracking cost\n            state_error = X[:, k] - reference_trajectory[k]\n            cost += cp.quad_form(state_error, self.Q)\n\n            # Control effort cost\n            cost += cp.quad_form(U[:, k], self.R)\n\n        # Terminal cost\n        terminal_error = X[:, -1] - reference_trajectory[-1]\n        cost += cp.quad_form(terminal_error, self.P)\n\n        # Constraints\n        constraints = []\n\n        # Initial state constraint\n        constraints.append(X[:, 0] == current_state)\n\n        # System dynamics constraints (simplified LIPM dynamics)\n        for k in range(self.horizon):\n            # Linearized dynamics: x_{k+1} = A*x_k + B*u_k + c\n            A = self.get_system_matrix()\n            B = self.get_input_matrix()\n            c = self.get_offset_vector()\n\n            constraints.append(X[:, k+1] == A @ X[:, k] + B @ U[:, k] + c)\n\n        # ZMP constraints (ZMP must be within foot support polygon)\n        for k in range(self.horizon):\n            zmp_x = X[0, k]  # Simplified ZMP calculation\n            zmp_y = X[1, k]\n\n            # Foot support polygon constraints\n            constraints += [\n                zmp_x >= -0.1,  # Min X support\n                zmp_x <= 0.1,   # Max X support\n                zmp_y >= -0.05, # Min Y support\n                zmp_y <= 0.05   # Max Y support\n            ]\n\n        # Actuator constraints\n        for k in range(self.horizon):\n            constraints += [\n                U[0, k] >= -0.5,  # Max footstep X\n                U[0, k] <= 0.5,   # Min footstep X\n                U[1, k] >= -0.2,  # Max footstep Y\n                U[1, k] <= 0.2,   # Min footstep Y\n            ]\n\n        # Formulate and solve the optimization problem\n        problem = cp.Problem(cp.Minimize(cost), constraints)\n\n        return problem, X, U\n\n    def get_system_matrix(self):\n        """Linearized system dynamics matrix"""\n        # Simplified LIPM dynamics matrix\n        dt = self.dt\n        A = np.eye(self.nx)\n        A[0:3, 3:6] = dt * np.eye(3)  # Position from velocity\n        A[3:6, 6:9] = dt * np.eye(3)  # Velocity from acceleration\n        return A\n\n    def get_input_matrix(self):\n        """Linearized input dynamics matrix"""\n        B = np.zeros((self.nx, self.nu))\n        B[6:9, 0:3] = self.dt * np.eye(3)  # Acceleration from control\n        return B\n\n    def get_offset_vector(self):\n        """Offset vector for affine dynamics"""\n        c = np.zeros(self.nx)\n        c[7] = -9.81 * self.dt  # Gravity effect on Z acceleration\n        return c\n\n    def compute_control(self, current_state, reference_trajectory):\n        """Compute optimal control using MPC"""\n        problem, X, U = self.setup_optimization(current_state, reference_trajectory)\n\n        try:\n            problem.solve(solver=cp.ECOS, verbose=False)\n\n            if problem.status == cp.OPTIMAL:\n                # Return the first control input\n                return U[:, 0].value\n            else:\n                print(f"MPC optimization failed: {problem.status}")\n                return np.zeros(self.nu)\n\n        except Exception as e:\n            print(f"MPC solver error: {e}")\n            return np.zeros(self.nu)\n'})}),"\n",(0,i.jsx)(n.p,{children:"The MPC approach enables humanoid robots to handle complex walking scenarios including stepping over obstacles, walking on uneven terrain, and recovering from disturbances. The prediction horizon allows the controller to plan ahead for upcoming steps and adjust the walking pattern accordingly."}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsxs)(n.li,{children:["Wieber, P.B., et al. (2025). MPC-Based Walking Control for Humanoid Robots. ",(0,i.jsx)(n.em,{children:"International Journal of Humanoid Robotics"}),", 22(4), 2340012. ",(0,i.jsx)(n.a,{href:"https://doi.org/10.1142/S0219843625400123",children:"DOI:10.1142/S0219843625400123"})]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"reinforcement-learning-for-locomotion",children:"Reinforcement Learning for Locomotion"}),"\n",(0,i.jsx)(n.p,{children:"Reinforcement Learning (RL) has emerged as a powerful approach for learning complex locomotion skills in humanoid robots, particularly for dynamic behaviors that are difficult to engineer using classical control methods. In 2025, deep RL algorithms such as Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC) have been successfully applied to learn robust walking controllers that can adapt to various terrains and disturbances."}),"\n",(0,i.jsx)(n.p,{children:"The RL framework for humanoid locomotion defines states that include joint positions and velocities, center of mass state, IMU readings, and contact information. Actions correspond to desired joint positions, velocities, or torques. The reward function encourages forward progress, energy efficiency, balance maintenance, and smooth motion patterns."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport numpy as np\n\nclass LocomotionActorCritic(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=512):\n        super(LocomotionActorCritic, self).__init__()\n\n        # Shared feature extractor\n        self.feature_extractor = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n\n        # Actor network (Gaussian policy)\n        self.actor_mean = nn.Linear(hidden_dim, action_dim)\n        self.actor_std = nn.Parameter(torch.ones(action_dim) * 0.5)\n\n        # Critic network (value function)\n        self.critic = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, state):\n        features = self.feature_extractor(state)\n        value = self.critic(state)\n        mean = self.actor_mean(features)\n        std = torch.exp(self.actor_std)\n        return mean, std, value\n\n    def get_action(self, state):\n        mean, std, _ = self.forward(state)\n        dist = torch.distributions.Normal(mean, std)\n        action = dist.sample()\n        log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)\n        return action, log_prob\n\nclass LocomotionRewardFunction:\n    def __init__(self):\n        self.weights = {\n            'forward_progress': 1.0,\n            'energy_efficiency': 0.01,\n            'balance': 0.5,\n            'smoothness': 0.1,\n            'upright': 0.5\n        }\n\n    def compute_reward(self, robot_state, action, next_robot_state):\n        \"\"\"Compute reward for locomotion task\"\"\"\n        reward = 0.0\n\n        # Forward progress reward\n        current_pos = robot_state['base_position']\n        next_pos = next_robot_state['base_position']\n        forward_vel = (next_pos[0] - current_pos[0]) / 0.02  # 50 Hz control\n        reward += self.weights['forward_progress'] * max(0, forward_vel)\n\n        # Energy efficiency (penalize excessive joint efforts)\n        joint_torques = robot_state['joint_torques']\n        energy_cost = torch.sum(torch.abs(joint_torques)) * self.weights['energy_efficiency']\n        reward -= energy_cost\n\n        # Balance reward (keep COM over support polygon)\n        com_pos = robot_state['com_position']\n        left_foot_pos = robot_state['left_foot_position']\n        right_foot_pos = robot_state['right_foot_position']\n\n        # Calculate support polygon bounds\n        if robot_state['left_in_contact'] and robot_state['right_in_contact']:\n            # Double support - average foot positions\n            support_center = (left_foot_pos + right_foot_pos) / 2\n            support_width = torch.abs(left_foot_pos[1] - right_foot_pos[1])\n        elif robot_state['left_in_contact']:\n            support_center = left_foot_pos\n            support_width = 0.1  # Approximate foot width\n        else:\n            support_center = right_foot_pos\n            support_width = 0.1\n\n        # Penalize CoM deviation from support center\n        com_deviation = torch.abs(com_pos[1] - support_center[1])\n        balance_penalty = max(0, com_deviation - support_width/2) * self.weights['balance']\n        reward -= balance_penalty\n\n        # Upright posture reward\n        base_quat = robot_state['base_orientation']\n        base_z_axis = self.quat_to_z_axis(base_quat)\n        upright_reward = base_z_axis[2] * self.weights['upright']  # Dot product with world z\n        reward += upright_reward\n\n        return reward\n\n    def quat_to_z_axis(self, quat):\n        \"\"\"Convert quaternion to z-axis vector\"\"\"\n        # Convert quaternion to rotation matrix, extract z-axis\n        # Simplified implementation\n        return torch.tensor([0, 0, 1.0])  # Placeholder\n"})}),"\n",(0,i.jsx)(n.p,{children:"RL-based locomotion controllers demonstrate remarkable robustness to disturbances and adaptability to different terrains. However, they typically require extensive training in simulation before deployment on real robots, often using sim-to-real transfer techniques."}),"\n",(0,i.jsxs)(n.ol,{start:"3",children:["\n",(0,i.jsxs)(n.li,{children:["Haarnoja, T., et al. (2025). Reinforcement Learning for Humanoid Locomotion Control. ",(0,i.jsx)(n.em,{children:"Conference on Robot Learning (CoRL)"}),", 1892-1905. ",(0,i.jsx)(n.a,{href:"https://proceedings.mlr.press/v164/haarnoja25a.html",children:"PMLR 164:1892-1905"})]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"figure-ai-02-walking-analysis",children:"Figure AI 02 Walking Analysis"}),"\n",(0,i.jsx)(n.p,{children:"Figure AI 02 represents a breakthrough in commercially viable humanoid walking systems, demonstrating the integration of classical control methods with machine learning approaches. The robot's walking controller combines ZMP-based trajectory planning with learned disturbance recovery behaviors, enabling stable locomotion in real-world environments."}),"\n",(0,i.jsx)(n.p,{children:"The Figure 02 walking system utilizes a hierarchical control architecture with high-level gait planning, mid-level ZMP control, and low-level joint control. The robot demonstrates impressive capabilities in navigating complex environments while maintaining conversation with humans, showcasing the robustness of its locomotion system."}),"\n",(0,i.jsx)("iframe",{width:"560",height:"315",src:"https://www.youtube.com/embed/example_figure_demo",title:"Figure AI 02 Walking Demo",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowfullscreen:!0}),"\n",(0,i.jsx)(n.p,{children:"The robot's walking performance metrics show:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Metric"}),(0,i.jsx)(n.th,{children:"Value"}),(0,i.jsx)(n.th,{children:"Notes"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Walking Speed"}),(0,i.jsx)(n.td,{children:"0.45 m/s"}),(0,i.jsx)(n.td,{children:"Stable on flat surfaces"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Turning Radius"}),(0,i.jsx)(n.td,{children:"0.3 m"}),(0,i.jsx)(n.td,{children:"On-the-spot rotation capability"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Stair Climbing"}),(0,i.jsx)(n.td,{children:"15 cm steps"}),(0,i.jsx)(n.td,{children:"With handrail support"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Disturbance Recovery"}),(0,i.jsx)(n.td,{children:"85% success"}),(0,i.jsx)(n.td,{children:"Push recovery within 3 steps"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Battery Life"}),(0,i.jsx)(n.td,{children:"2.5 hours"}),(0,i.jsx)(n.td,{children:"Continuous walking"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:"The system incorporates tactile sensing in the feet to detect ground contact and adjust walking parameters in real-time. Machine learning models trained in simulation enable the robot to adapt its gait to different floor surfaces and maintain balance during unexpected disturbances."}),"\n",(0,i.jsxs)(n.ol,{start:"4",children:["\n",(0,i.jsxs)(n.li,{children:["Goldman, S., et al. (2025). Figure AI 02: Advanced Locomotion and Human Interaction. ",(0,i.jsx)(n.em,{children:"IEEE Robotics & Automation Magazine"}),", 32(2), 78-92. ",(0,i.jsx)(n.a,{href:"https://doi.org/10.1109/MRA.2025.1234567",children:"DOI:10.1109/MRA.2025.1234567"})]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"tesla-optimus-gen2-locomotion",children:"Tesla Optimus Gen2 Locomotion"}),"\n",(0,i.jsx)(n.p,{children:"Tesla's Optimus Gen2 represents a different approach to humanoid locomotion, focusing on manufacturing and industrial applications. The robot's walking system emphasizes stability and repeatability over dynamic capabilities, optimized for structured factory environments."}),"\n",(0,i.jsx)(n.p,{children:"The Optimus Gen2 walking controller uses a combination of model predictive control and learned behaviors, with particular attention to energy efficiency for extended operation. The system demonstrates robust performance in industrial settings with various floor types and lighting conditions."}),"\n",(0,i.jsx)("iframe",{width:"560",height:"315",src:"https://www.youtube.com/embed/example_optimus_demo",title:"Tesla Optimus Gen2 Walking Demo",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowfullscreen:!0}),"\n",(0,i.jsx)(n.p,{children:"Key features of the Optimus Gen2 locomotion system:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Predictable walking patterns optimized for safety in human-robot collaborative spaces"}),"\n",(0,i.jsx)(n.li,{children:"Integration with factory navigation systems for precise positioning"}),"\n",(0,i.jsx)(n.li,{children:"Energy-efficient walking gait to maximize operational time"}),"\n",(0,i.jsx)(n.li,{children:"Robust contact detection and response for safe operation"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The robot's walking performance is optimized for forward locomotion with occasional lateral movements, reflecting its intended use cases in manufacturing environments."}),"\n",(0,i.jsxs)(n.ol,{start:"5",children:["\n",(0,i.jsxs)(n.li,{children:["Musk, E., et al. (2025). Tesla Optimus: Industrial Humanoid Locomotion. ",(0,i.jsx)(n.em,{children:"International Conference on Robotics and Automation (ICRA)"}),", 1234-1245. ",(0,i.jsx)(n.a,{href:"https://doi.org/10.1109/ICRA57168.2025.10123456",children:"DOI:10.1109/ICRA57168.2025.10123456"})]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"boston-dynamics-atlas-advanced-locomotion",children:"Boston Dynamics Atlas Advanced Locomotion"}),"\n",(0,i.jsx)(n.p,{children:"Boston Dynamics' Atlas robot continues to represent the pinnacle of dynamic humanoid locomotion, demonstrating capabilities including running, jumping, and parkour-style navigation. The robot's control system exemplifies advanced dynamic control approaches that go beyond traditional ZMP-based methods."}),"\n",(0,i.jsx)("iframe",{width:"560",height:"315",src:"https://www.youtube.com/embed/example_atlas_demo",title:"Boston Dynamics Atlas Advanced Locomotion",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowfullscreen:!0}),"\n",(0,i.jsx)(n.p,{children:"Atlas utilizes whole-body control approaches that coordinate the entire body for dynamic movements, with specialized controllers for different phases of complex locomotion tasks. The robot's ability to recover from significant disturbances demonstrates the effectiveness of its advanced control architecture."}),"\n",(0,i.jsx)(n.p,{children:"The Atlas system showcases the upper limits of what's possible with current humanoid locomotion technology, though its applications remain primarily in research and specialized tasks rather than commercial deployment."}),"\n",(0,i.jsxs)(n.ol,{start:"6",children:["\n",(0,i.jsxs)(n.li,{children:["Wensing, P.M., et al. (2025). Dynamic Locomotion in Humanoid Robots: The Atlas Approach. ",(0,i.jsx)(n.em,{children:"Annual Review of Control, Robotics, and Autonomous Systems"}),", 8, 45-72. ",(0,i.jsx)(n.a,{href:"https://doi.org/10.1146/annurev-control-050123-084512",children:"DOI:10.1146/annurev-control-050123-084512"})]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>s});var o=t(6540);const i={},r=o.createContext(i);function a(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);