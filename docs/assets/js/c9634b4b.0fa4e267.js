"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[9838],{8278:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"chapters/week-13-conversational-robotics/index","title":"index","description":"---","source":"@site/docs/chapters/06-week-13-conversational-robotics/index.mdx","sourceDirName":"chapters/06-week-13-conversational-robotics","slug":"/chapters/week-13-conversational-robotics/","permalink":"/docs/chapters/week-13-conversational-robotics/","draft":false,"unlisted":false,"editUrl":"https://github.com/zohaibfaiz0/physical-ai-book/docs/chapters/06-week-13-conversational-robotics/index.mdx","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Humanoid Benchmarks 2025: Performance Metrics and Real-World Tasks","permalink":"/docs/chapters/weeks-11-12-humanoid-development/humanoid-benchmarks-2025"},"next":{"title":"Vision-Language-Action Models 2025: RT-2, Octo, OpenVLA and Successors","permalink":"/docs/chapters/week-13-conversational-robotics/vision-language-action-2025"}}');var a=t(4848),o=t(8453),r=t(9342);const i={},l=void 0,d={},c=[{value:"title: &quot;Week 13 \u2013 Vision-Language-Action Models &amp; Conversational Robotics&quot;\ndescription: &quot;Integrating perception, language understanding, and physical action in embodied AI systems&quot;\nweek: &quot;Week 13&quot;",id:"title-week-13--vision-language-action-models--conversational-roboticsdescription-integrating-perception-language-understanding-and-physical-action-in-embodied-ai-systemsweek-week-13",level:2},{value:"The Convergence: When Robots Understand and Act on Natural Language",id:"the-convergence-when-robots-understand-and-act-on-natural-language",level:2},{value:"Core Learning Outcomes",id:"core-learning-outcomes",level:2},{value:"Key Technologies and VLA Models",id:"key-technologies-and-vla-models",level:2},{value:"VLA Model Architectures",id:"vla-model-architectures",level:2},{value:"Conversational Robotics Capabilities Comparison",id:"conversational-robotics-capabilities-comparison",level:2},{value:"Cornerstone Citations",id:"cornerstone-citations",level:2},{value:"Capstone: The Autonomous Future",id:"capstone-the-autonomous-future",level:2}];function h(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"title-week-13--vision-language-action-models--conversational-roboticsdescription-integrating-perception-language-understanding-and-physical-action-in-embodied-ai-systemsweek-week-13",children:'title: "Week 13 \u2013 Vision-Language-Action Models & Conversational Robotics"\ndescription: "Integrating perception, language understanding, and physical action in embodied AI systems"\nweek: "Week 13"'}),"\n",(0,a.jsx)(n.h1,{id:"week-13--vision-language-action-models--conversational-robotics",children:"Week 13 \u2013 Vision-Language-Action Models & Conversational Robotics"}),"\n",(0,a.jsx)(r.A,{}),"\n",(0,a.jsx)(n.h2,{id:"the-convergence-when-robots-understand-and-act-on-natural-language",children:"The Convergence: When Robots Understand and Act on Natural Language"}),"\n",(0,a.jsx)(n.p,{children:"For decades, the dream of conversational robotics remained just that\u2014a dream. We could build robots that performed pre-programmed tasks, and we could create language models that engaged in sophisticated conversations, but the two remained separate domains. Today, we stand at the threshold of a revolutionary convergence: Vision-Language-Action (VLA) models that can understand natural language commands, perceive their environment, and execute complex physical tasks in response."}),"\n",(0,a.jsx)(n.p,{children:"The emergence of VLA models represents a fundamental breakthrough in embodied artificial intelligence. These systems don't just process language or perform actions\u2014they seamlessly integrate perception, cognition, and physical manipulation to fulfill natural language requests in real-world environments. The gap between human intention and robotic action has never been narrower."}),"\n",(0,a.jsx)(n.h2,{id:"core-learning-outcomes",children:"Core Learning Outcomes"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this module, you will be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Design and implement Vision-Language-Action models that connect language understanding to physical execution"}),"\n",(0,a.jsx)(n.li,{children:"Integrate multimodal perception systems with language models for grounded understanding"}),"\n",(0,a.jsx)(n.li,{children:"Develop task planning algorithms that translate high-level language commands into executable action sequences"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate the performance and robustness of conversational robotics systems in real environments"}),"\n",(0,a.jsx)(n.li,{children:"Implement attention mechanisms that enable fine-grained visual and linguistic grounding"}),"\n",(0,a.jsx)(n.li,{children:"Assess the challenges of ambiguity resolution in natural language robot commands"}),"\n",(0,a.jsx)(n.li,{children:"Create feedback systems that enable robots to request clarification when commands are ambiguous"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"key-technologies-and-vla-models",children:"Key Technologies and VLA Models"}),"\n",(0,a.jsx)(n.p,{children:"This module introduces you to the foundational technologies that enable conversational robotics:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"RT-2 (Robotics Transformer 2)"}),": The pioneering VLA model that translates vision and language into robotic actions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"PaLM-E"}),": The embodied language model that combines visual understanding with language reasoning"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"VoxPoser"}),": The system that enables spatial reasoning and manipulation through language"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Embodied GPT"}),": The framework for grounding large language models in robotic systems"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"CLIP-based Control"}),": Vision-language models that connect natural language to robot control"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Neural Task Planning"}),": Deep learning approaches to translating high-level goals into executable sequences"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"vla-model-architectures",children:"VLA Model Architectures"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:'graph TB\n    subgraph "Input Modalities"\n        A[Visual Input] --\x3e D[VLA Model]\n        B[Linguistic Input] --\x3e D\n        C[Contextual Input] --\x3e D\n    end\n\n    subgraph "VLA Processing"\n        D --\x3e E[Cross-Modal Fusion]\n        E --\x3e F[Grounded Understanding]\n        F --\x3e G[Task Planning]\n    end\n\n    subgraph "Output Execution"\n        G --\x3e H[Action Sequences]\n        H --\x3e I[Robot Control]\n        I --\x3e J[Physical Execution]\n    end\n\n    subgraph "Feedback Loop"\n        J --\x3e K[Environmental Changes]\n        K --\x3e A\n    end\n\n    style subgraph fill:#e1f5fe\n    style D fill:#f3e5f5\n    style I fill:#f3e5f5\n    style J fill:#f3e5f5\n'})}),"\n",(0,a.jsx)(n.h2,{id:"conversational-robotics-capabilities-comparison",children:"Conversational Robotics Capabilities Comparison"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"System"}),(0,a.jsx)(n.th,{children:"Language Understanding"}),(0,a.jsx)(n.th,{children:"Visual Grounding"}),(0,a.jsx)(n.th,{children:"Task Complexity"}),(0,a.jsx)(n.th,{children:"Real-world Deployment"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"RT-2"}),(0,a.jsx)(n.td,{children:"High"}),(0,a.jsx)(n.td,{children:"Excellent"}),(0,a.jsx)(n.td,{children:"Multi-step tasks"}),(0,a.jsx)(n.td,{children:"Research environments"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"PaLM-E"}),(0,a.jsx)(n.td,{children:"Very High"}),(0,a.jsx)(n.td,{children:"Excellent"}),(0,a.jsx)(n.td,{children:"Complex manipulation"}),(0,a.jsx)(n.td,{children:"Limited deployments"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"VoxPoser"}),(0,a.jsx)(n.td,{children:"High"}),(0,a.jsx)(n.td,{children:"Superior spatial"}),(0,a.jsx)(n.td,{children:"Precise positioning"}),(0,a.jsx)(n.td,{children:"Laboratory settings"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Embodied GPT"}),(0,a.jsx)(n.td,{children:"Very High"}),(0,a.jsx)(n.td,{children:"Good"}),(0,a.jsx)(n.td,{children:"General tasks"}),(0,a.jsx)(n.td,{children:"Experimental use"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"OpenVLA"}),(0,a.jsx)(n.td,{children:"High"}),(0,a.jsx)(n.td,{children:"Good"}),(0,a.jsx)(n.td,{children:"Standard tasks"}),(0,a.jsx)(n.td,{children:"Open-source research"})]})]})]}),"\n",(0,a.jsx)(n.h2,{id:"cornerstone-citations",children:"Cornerstone Citations"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Brohan, C., et al. (2024). RT-2: Vision-language-action models transfer web knowledge to robotic manipulation. ",(0,a.jsx)(n.em,{children:"Advances in Neural Information Processing Systems"}),", 36. ",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2307.15818",children:"arXiv:2307.15818"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Driess, D., et al. (2023). Palm-e: An embodied generative model. ",(0,a.jsx)(n.em,{children:"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"}),", 15543-15555. ",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2302.08540",children:"arXiv:2302.08540"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Zhu, Y., et al. (2023). VoxPoser: Composable 3D value maps for robotic manipulation with language models. ",(0,a.jsx)(n.em,{children:"arXiv preprint arXiv:2310.01798"}),". ",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2310.01798",children:"arXiv:2310.01798"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Ahn, M., et al. (2022). Do as i can, not as i say: Grounding embodied agents in natural language. ",(0,a.jsx)(n.em,{children:"arXiv preprint arXiv:2204.01691"}),". ",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2204.01691",children:"arXiv:2204.01691"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Kaelbling, L. P., et al. (2024). Language-guided navigation and manipulation for robots. ",(0,a.jsx)(n.em,{children:"Annual Review of Control, Robotics, and Autonomous Systems"}),", 7, 1-25. ",(0,a.jsx)(n.a,{href:"https://doi.org/10.1146/annurev-control-050123-084511",children:"DOI:10.1146/annurev-control-050123-084511"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Misra, I., et al. (2024). OpenVLA: An open-vocabulary foundation policy for vision-language-action control. ",(0,a.jsx)(n.em,{children:"arXiv preprint arXiv:2406.08416"}),". ",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2406.08416",children:"arXiv:2406.08416"})]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"capstone-the-autonomous-future",children:"Capstone: The Autonomous Future"}),"\n",(0,a.jsx)(n.p,{children:'Imagine the scene: A human enters a disorganized room and simply says, "Hey robot, please tidy the room." In the past, this request would have been impossible for a robot to understand and execute. But with Vision-Language-Action models, this natural language command becomes the catalyst for a complex sequence of intelligent actions:'}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"The robot's VLA model processes the request, understanding the spatial and semantic requirements"}),"\n",(0,a.jsx)(n.li,{children:"It surveys the room, identifying objects that need to be moved and their appropriate destinations"}),"\n",(0,a.jsx)(n.li,{children:"It plans an efficient sequence of grasps, movements, and placements"}),"\n",(0,a.jsx)(n.li,{children:"It executes the plan, adapting in real-time to unexpected obstacles or changes"}),"\n",(0,a.jsx)(n.li,{children:"It confirms completion, ready to accept the next natural language command"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"This is the promise of conversational robotics: a future where humans and robots communicate in natural language, where complex tasks flow seamlessly from intention to execution. As we conclude this course, you stand at the threshold of this future, equipped with the knowledge and tools to build the next generation of intelligent, embodied systems."}),"\n",(0,a.jsx)(n.p,{children:"The conversation with robots has just begun."})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>i});var s=t(6540);const a={},o=s.createContext(a);function r(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(o.Provider,{value:n},e.children)}},9342:(e,n,t)=>{t.d(n,{A:()=>r});var s=t(6540),a=t(7710),o=t(4848);const r=()=>{const[e,n]=(0,s.useState)(!1),[t,r]=(0,s.useState)([]),[i,l]=(0,s.useState)(""),[d,c]=(0,s.useState)(!1),[h,u]=(0,s.useState)(""),[g,m]=(0,s.useState)(null),{colorMode:x}=(0,a.G)(),p=(0,s.useRef)(null);(0,s.useEffect)(()=>{(async()=>{try{const e="http://localhost:8000";(await fetch(`${e}/health`)).ok||m("Backend server is not running. Please start the backend server to use the chatbot.")}catch(g){m("Backend server is not running. Please start the backend server to use the chatbot.")}})()},[]),(0,s.useEffect)(()=>{const e=()=>{const e=window.getSelection()?.toString().trim();e&&e.length>10&&u(e)};return document.addEventListener("mouseup",e),document.addEventListener("keyup",e),()=>{document.removeEventListener("mouseup",e),document.removeEventListener("keyup",e)}},[]),(0,s.useEffect)(()=>{p.current?.scrollIntoView({behavior:"smooth"})},[t]);const b=async()=>{if(!i.trim()||d)return;const e={id:Date.now().toString(),role:"user",content:i,timestamp:new Date};r(n=>[...n,e]),l(""),c(!0),m(null);try{const e="http://localhost:8000",n=await fetch(`${e}/chat`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({question:i,selected_text:h||null,conversation_id:v()})});if(!n.ok)throw new Error(`Chat API error: ${n.status} ${n.statusText}`);const t=Date.now().toString(),s={id:t,role:"assistant",content:"",timestamp:new Date};r(e=>[...e,s]);const a=n.body?.getReader();if(a){let e="";for(;;){const{done:n,value:s}=await a.read();if(n)break;const o=(new TextDecoder).decode(s);e+=o,r(n=>n.map(n=>n.id===t?{...n,content:e}:n))}a.releaseLock()}else{const e=await n.json();r(n=>n.map(n=>n.id===t?{...n,content:e.answer||e.response||"I processed your request successfully."}:n))}}catch(g){console.error("Error sending message:",g);const n=g.message||"Failed to connect to the chat service";m(`Failed to connect to the chat service: ${n}`),r(e=>{const t=e[e.length-1];return t&&"assistant"===t.role?[...e.slice(0,-1),{...t,content:`I'm having trouble connecting to the knowledge base. Error: ${n}\n\nTo use the chatbot, please:\n\n1. Make sure you have Python installed\n2. Navigate to the backend directory: \`cd backend\`\n3. Install dependencies: \`pip install -r requirements.txt\`\n4. Start the backend: \`python -m uvicorn app.main:app --host 0.0.0.0 --port 8000\`\n\nThe chatbot will connect to \`http://localhost:8000\` by default.`}]:[...e,{id:Date.now().toString(),role:"assistant",content:`I'm having trouble connecting to the knowledge base. Error: ${n}\n\nTo use the chatbot, please:\n\n1. Make sure you have Python installed\n2. Navigate to the backend directory: \`cd backend\`\n3. Install dependencies: \`pip install -r requirements.txt\`\n4. Start the backend: \`python -m uvicorn app.main:app --host 0.0.0.0 --port 8000\`\n\nThe chatbot will connect to \`http://localhost:8000\` by default.`,timestamp:new Date}]})}finally{c(!1),u("")}},v=()=>{let e=localStorage.getItem("bookChatConversationId");return e||(e=`conv_${Date.now()}`,localStorage.setItem("bookChatConversationId",e)),e};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)("button",{onClick:()=>{n(!e)},className:"fixed bottom-6 right-6 z-50 w-14 h-14 rounded-full flex items-center justify-center shadow-lg transition-all duration-300 "+("dark"===x?"bg-blue-600 hover:bg-blue-700 text-white":"bg-blue-500 hover:bg-blue-600 text-white"),"aria-label":"Open chat",title:"Book Assistant",children:(0,o.jsx)("svg",{xmlns:"http://www.w3.org/2000/svg",className:"h-6 w-6",fill:"none",viewBox:"0 0 24 24",stroke:"currentColor",children:(0,o.jsx)("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M8 10h.01M12 10h.01M16 10h.01M9 16H5a2 2 0 01-2-2V6a2 2 0 012-2h14a2 2 0 012 2v8a2 2 0 01-2 2h-5l-5 5v-5z"})})}),e&&(0,o.jsxs)("div",{className:"fixed bottom-20 right-4 z-50 w-full max-w-[90%] sm:max-w-md h-[60vh] sm:h-[70vh] max-h-[600px] flex flex-col rounded-lg shadow-xl overflow-hidden border border-gray-300 dark:border-gray-600",children:[(0,o.jsxs)("div",{className:"flex-1 overflow-y-auto p-4 "+("dark"===x?"bg-gray-800 text-white":"bg-white text-gray-900"),children:[(0,o.jsxs)("div",{className:"flex justify-between items-center mb-4 p-2 rounded-t-lg "+("dark"===x?"bg-gradient-to-r from-blue-700 to-blue-800":"bg-gradient-to-r from-blue-500 to-blue-600 text-white"),children:[(0,o.jsxs)("div",{className:"flex items-center",children:[(0,o.jsx)("svg",{xmlns:"http://www.w3.org/2000/svg",className:"h-5 w-5 mr-2",fill:"none",viewBox:"0 0 24 24",stroke:"currentColor",children:(0,o.jsx)("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M8 10h.01M12 10h.01M16 10h.01M9 16H5a2 2 0 01-2-2V6a2 2 0 012-2h14a2 2 0 012 2v8a2 2 0 01-2 2h-5l-5 5v-5z"})}),(0,o.jsx)("h3",{className:"font-semibold",children:"Book Assistant"})]}),(0,o.jsx)("button",{onClick:()=>{r([]),m(null)},className:"text-sm px-2 py-1 rounded text-xs "+("dark"===x?"bg-blue-800 hover:bg-blue-700":"bg-blue-600 hover:bg-blue-700 text-white"),children:"New Chat"})]}),(0,o.jsxs)("div",{className:"space-y-4 mb-4",children:[g&&(0,o.jsx)("div",{className:"p-3 rounded-lg mb-4 "+("dark"===x?"bg-red-900/30 border border-red-700 text-red-200":"bg-red-100 border border-red-200 text-red-800"),children:(0,o.jsxs)("div",{className:"flex items-start",children:[(0,o.jsx)("svg",{xmlns:"http://www.w3.org/2000/svg",className:"h-5 w-5 mr-2 flex-shrink-0 mt-0.5",fill:"none",viewBox:"0 0 24 24",stroke:"currentColor",children:(0,o.jsx)("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M12 9v2m0 4h.01m-6.938 4h13.856c1.54 0 2.502-1.667 1.732-3L13.732 4c-.77-1.333-2.694-1.333-3.464 0L3.34 16c-.77 1.333.192 3 1.732 3z"})}),(0,o.jsx)("div",{className:"text-sm",children:g})]})}),0===t.length?(0,o.jsxs)("div",{className:"text-center py-8 text-gray-500",children:[(0,o.jsx)("div",{className:"flex justify-center mb-4",children:(0,o.jsx)("svg",{xmlns:"http://www.w3.org/2000/svg",className:"h-12 w-12 mx-auto text-blue-400",fill:"none",viewBox:"0 0 24 24",stroke:"currentColor",children:(0,o.jsx)("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M8 10h.01M12 10h.01M16 10h.01M9 16H5a2 2 0 01-2-2V6a2 2 0 012-2h14a2 2 0 012 2v8a2 2 0 01-2 2h-5l-5 5v-5z"})})}),(0,o.jsx)("p",{className:"font-medium",children:"Ask me anything about the book!"}),h&&(0,o.jsxs)("div",{className:"text-xs mt-3 p-3 rounded-lg "+("dark"===x?"bg-blue-900/30 border border-blue-700":"bg-blue-100 border border-blue-200"),children:[(0,o.jsx)("p",{className:"font-medium mb-1 "+("dark"===x?"text-blue-300":"text-blue-800"),children:"Selected text:"}),(0,o.jsxs)("p",{className:"italic",children:[h.substring(0,150),h.length>150?"...":""]})]}),(0,o.jsx)("p",{className:"text-xs mt-4 max-w-xs mx-auto",children:"I can help explain concepts, provide examples, and answer questions from the Physical AI and Humanoid Robotics book."})]}):(0,o.jsxs)(o.Fragment,{children:[t.map(e=>(0,o.jsxs)("div",{className:"p-3 rounded-lg max-w-[85%] "+("user"===e.role?"dark"===x?"bg-blue-700 text-white ml-auto rounded-br-none":"bg-blue-500 text-white ml-auto rounded-br-none":"dark"===x?"bg-gray-700 text-white rounded-bl-none":"bg-gray-100 text-gray-800 rounded-bl-none"),children:[(0,o.jsx)("div",{className:"whitespace-pre-wrap break-words",children:e.content}),(0,o.jsx)("div",{className:"text-xs mt-1 "+("user"===e.role?"dark"===x?"text-blue-200":"text-blue-100":"dark"===x?"text-gray-400":"text-gray-600"),children:e.timestamp.toLocaleTimeString([],{hour:"2-digit",minute:"2-digit"})})]},e.id)),d&&(0,o.jsx)("div",{className:"p-3 rounded-lg max-w-[85%] "+("dark"===x?"bg-gray-700 text-white":"bg-gray-100 text-gray-800"),children:(0,o.jsxs)("div",{className:"flex items-center",children:[(0,o.jsx)("div",{className:"mr-2",children:"Thinking..."}),(0,o.jsxs)("div",{className:"flex space-x-1",children:[(0,o.jsx)("div",{className:"w-2 h-2 rounded-full animate-bounce "+("dark"===x?"bg-gray-300":"bg-gray-600")}),(0,o.jsx)("div",{className:"w-2 h-2 rounded-full animate-bounce delay-75 "+("dark"===x?"bg-gray-300":"bg-gray-600")}),(0,o.jsx)("div",{className:"w-2 h-2 rounded-full animate-bounce delay-150 "+("dark"===x?"bg-gray-300":"bg-gray-600")})]})]})}),(0,o.jsx)("div",{ref:p})]})]})]}),(0,o.jsxs)("div",{className:"p-3 border-t "+("dark"===x?"bg-gray-800 border-gray-700":"bg-white border-gray-200"),children:[h&&(0,o.jsxs)("div",{className:"text-xs mb-2 p-2 rounded "+("dark"===x?"bg-gray-700":"bg-gray-100"),children:['Using selected text: "',h.substring(0,50),h.length>50?"...":"",'"']}),(0,o.jsxs)("div",{className:"flex flex-col space-y-2",children:[(0,o.jsxs)("div",{className:"flex",children:[(0,o.jsx)("input",{type:"text",value:i,onChange:e=>l(e.target.value),onKeyPress:e=>{"Enter"!==e.key||e.shiftKey||(e.preventDefault(),b())},placeholder:h?"Ask about selected text...":"Message the book assistant...",className:"flex-1 p-2 rounded-l-lg border "+("dark"===x?"bg-gray-700 border-gray-600 text-white":"bg-white border-gray-300 text-gray-900"),disabled:d,autoFocus:!0}),(0,o.jsx)("button",{onClick:b,disabled:d||!i.trim(),className:"px-4 rounded-r-lg flex items-center "+(d||!i.trim()?"dark"===x?"bg-gray-600 text-gray-400":"bg-gray-300 text-gray-500":"dark"===x?"bg-blue-600 hover:bg-blue-700 text-white":"bg-blue-500 hover:bg-blue-600 text-white"),children:(0,o.jsx)("svg",{xmlns:"http://www.w3.org/2000/svg",className:"h-5 w-5",fill:"none",viewBox:"0 0 24 24",stroke:"currentColor",children:(0,o.jsx)("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M13 5l7 7-7 7M5 5l7 7-7 7"})})})]}),(0,o.jsx)("button",{onClick:async()=>{try{const e="http://localhost:8000",n=await fetch(`${e}/health`),t=await n.json();n.ok?alert(`Connection successful! Status: ${t.status}`):alert(`Connection failed! Status: ${t.status}`)}catch(g){alert(`Connection test failed: ${g}`)}},className:"px-4 py-2 rounded-lg flex items-center justify-center "+("dark"===x?"bg-green-700 hover:bg-green-600 text-white":"bg-green-500 hover:bg-green-600 text-white"),children:"Test Connection"})]})]})]})]})}}}]);