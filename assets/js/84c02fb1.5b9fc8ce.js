"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[9553],{6357:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>m,frontMatter:()=>s,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"chapters/week-13-conversational-robotics/capstone-full-autonomous-humanoid","title":"Capstone: Full Autonomous Humanoid Robot System","description":"Complete end-to-end capstone project for autonomous humanoid robot with conversational interface","source":"@site/docs/chapters/06-week-13-conversational-robotics/03-capstone-full-autonomous-humanoid.mdx","sourceDirName":"chapters/06-week-13-conversational-robotics","slug":"/chapters/week-13-conversational-robotics/capstone-full-autonomous-humanoid","permalink":"/physical-ai-book/docs/chapters/week-13-conversational-robotics/capstone-full-autonomous-humanoid","draft":false,"unlisted":false,"editUrl":"https://github.com/zohaibfaiz0/physical-ai-book/docs/chapters/06-week-13-conversational-robotics/03-capstone-full-autonomous-humanoid.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Capstone: Full Autonomous Humanoid Robot System","description":"Complete end-to-end capstone project for autonomous humanoid robot with conversational interface","week":"Week 13"},"sidebar":"tutorialSidebar","previous":{"title":"Voice-to-Action Pipeline: From Speech Recognition to ROS2 Actions","permalink":"/physical-ai-book/docs/chapters/week-13-conversational-robotics/voice-to-action-pipeline"},"next":{"title":"index","permalink":"/physical-ai-book/docs/chapters/appendices/"}}');var i=t(4848),a=t(8453);const s={title:"Capstone: Full Autonomous Humanoid Robot System",description:"Complete end-to-end capstone project for autonomous humanoid robot with conversational interface",week:"Week 13"},r="Capstone: Full Autonomous Humanoid Robot System",c={},l=[{value:"Architecture Overview and System Design",id:"architecture-overview-and-system-design",level:2},{value:"Task Implementation: &quot;Pick up the red cup from the table and place it in the sink&quot;",id:"task-implementation-pick-up-the-red-cup-from-the-table-and-place-it-in-the-sink",level:2},{value:"ROS Packages and Launch Configuration",id:"ros-packages-and-launch-configuration",level:2},{value:"Isaac Lab Integration and Setup",id:"isaac-lab-integration-and-setup",level:2},{value:"Evaluation Rubric and Success Criteria",id:"evaluation-rubric-and-success-criteria",level:2},{value:"Bonus Extensions and Advanced Features",id:"bonus-extensions-and-advanced-features",level:2},{value:"Demo Video and GitHub Repository",id:"demo-video-and-github-repository",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"capstone-full-autonomous-humanoid-robot-system",children:"Capstone: Full Autonomous Humanoid Robot System"})}),"\n",(0,i.jsx)(n.h2,{id:"architecture-overview-and-system-design",children:"Architecture Overview and System Design"}),"\n",(0,i.jsx)(n.p,{children:"The full autonomous humanoid robot system represents the culmination of all previous modules, integrating perception, reasoning, control, and interaction capabilities into a cohesive platform. The architecture follows a layered approach with clear interfaces between components, enabling modular development and testing while maintaining overall system coherence. The system design prioritizes safety, robustness, and extensibility, ensuring that the robot can operate reliably in real-world environments."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:'graph TB\n    subgraph "User Interface Layer"\n        A[Voice Command Input]\n        B[Tactile Input]\n        C[Gesture Recognition]\n    end\n\n    subgraph "Natural Language Processing Layer"\n        D[Whisper ASR]\n        E[LLM Reasoning]\n        F[Command Parser]\n    end\n\n    subgraph "Perception Layer"\n        G[RGB-D Camera]\n        H[LIDAR]\n        I[IMU/Sensors]\n        J[Object Detection]\n        K[Semantic Segmentation]\n    end\n\n    subgraph "Planning and Control Layer"\n        L[Task Planner]\n        M[Path Planner]\n        N[Whole Body Controller]\n        O[Motion Generator]\n    end\n\n    subgraph "Execution Layer"\n        P[Navigation Action]\n        Q[Manipulation Action]\n        R[Speech Synthesis]\n        S[Humanoid Platform]\n    end\n\n    subgraph "Knowledge and Memory Layer"\n        T[World Model]\n        U[Object Database]\n        V[Task Library]\n        W[Experience Buffer]\n    end\n\n    A --\x3e D\n    B --\x3e F\n    C --\x3e E\n    D --\x3e E\n    E --\x3e F\n    G --\x3e J\n    H --\x3e M\n    I --\x3e N\n    J --\x3e T\n    K --\x3e T\n    F --\x3e L\n    L --\x3e M\n    L --\x3e O\n    M --\x3e P\n    O --\x3e Q\n    Q --\x3e S\n    P --\x3e S\n    R --\x3e A\n    T --\x3e L\n    U --\x3e J\n    V --\x3e L\n    W --\x3e E\n'})}),"\n",(0,i.jsx)(n.p,{children:'The architecture is designed to handle the specific task of "Pick up the red cup from the table and place it in the sink" while providing the flexibility to extend to more complex scenarios. The system operates in a continuous loop of perception, reasoning, planning, and execution, with feedback mechanisms that allow for adaptation to changing conditions and error recovery.'}),"\n",(0,i.jsx)(n.p,{children:"The core components of the system include:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Input Processing"}),": Handles voice commands, gesture recognition, and environmental sensing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Language Understanding"}),": Interprets natural language commands and extracts actionable intents"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Perception System"}),": Processes sensor data to understand the environment and locate objects"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Task Planning"}),": Decomposes high-level commands into executable action sequences"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Motion Control"}),": Generates and executes precise robot motions for navigation and manipulation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feedback System"}),": Monitors execution and provides status updates to users"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety System"}),": Ensures safe operation and handles emergency situations"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The system design emphasizes modularity and fault tolerance. Each component operates as an independent ROS2 node with well-defined interfaces, allowing for easy testing, debugging, and replacement. The use of ROS2 actions for long-running tasks ensures that operations can be monitored, preempted, and recovered from failures gracefully."}),"\n",(0,i.jsx)(n.p,{children:"Communication between components follows ROS2 best practices with appropriate Quality of Service (QoS) settings for different types of data. Time-sensitive control commands use reliable delivery with high priority, while perception results can tolerate some loss for better performance. The system incorporates extensive logging and monitoring capabilities to facilitate debugging and performance analysis."}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Siciliano, B., & Khatib, O. (2025). Springer Handbook of Robotics: Autonomous Humanoid Systems. ",(0,i.jsx)(n.em,{children:"Springer International Publishing"}),", 2nd Edition, Chapter 45. ",(0,i.jsx)(n.a,{href:"https://doi.org/10.1007/978-3-030-97610-9_45",children:"DOI:10.1007/978-3-030-97610-9_45"})]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"task-implementation-pick-up-the-red-cup-from-the-table-and-place-it-in-the-sink",children:'Task Implementation: "Pick up the red cup from the table and place it in the sink"'}),"\n",(0,i.jsx)(n.p,{children:'The implementation of the core task "Pick up the red cup from the table and place it in the sink" involves a sophisticated sequence of perception, planning, and control operations. The system must identify the red cup among potentially many objects, determine its precise location relative to the robot and the environment, plan a safe path to reach the cup, execute a stable grasp, transport the cup to the sink, and place it appropriately.'}),"\n",(0,i.jsx)(n.p,{children:"The task implementation follows these key steps:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Object Identification and Localization"}),': The system uses computer vision to identify the red cup in the environment. This involves detecting objects in the RGB-D camera data, classifying them, and matching the detected objects against the requested "red cup" specification. The system must handle variations in lighting, occlusion, and appearance while maintaining accuracy.']}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import cv2\nimport numpy as np\nfrom scipy.spatial import distance\nimport rospy\nfrom sensor_msgs.msg import Image, PointCloud2\nfrom geometry_msgs.msg import PointStamped\nimport tf2_ros\nfrom visualization_msgs.msg import Marker, MarkerArray\n\nclass ObjectIdentifier:\n    def __init__(self):\n        # Initialize perception components\n        self.tf_buffer = tf2_ros.Buffer()\n        self.tf_listener = tf2_ros.TransformListener(self.tf_buffer)\n\n        # Initialize object detection model\n        self.detection_model = self.load_detection_model()\n\n        # Subscribe to sensor data\n        self.image_sub = rospy.Subscriber('/camera/rgb/image_raw', Image, self.image_callback)\n        self.depth_sub = rospy.Subscriber('/camera/depth/image_raw', Image, self.depth_callback)\n        self.pc_sub = rospy.Subscriber('/camera/depth/points', PointCloud2, self.pointcloud_callback)\n\n        # Publishers for visualization\n        self.marker_pub = rospy.Publisher('/object_markers', MarkerArray, queue_size=10)\n        self.target_pub = rospy.Publisher('/target_object', PointStamped, queue_size=10)\n\n        # Internal state\n        self.latest_image = None\n        self.latest_depth = None\n        self.objects = {}\n\n        rospy.loginfo(\"Object Identifier initialized\")\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming RGB image\"\"\"\n        # Convert ROS image to OpenCV format\n        self.latest_image = self.ros_to_cv2(msg)\n\n        # Perform object detection\n        detections = self.detect_objects(self.latest_image)\n\n        # Update object database\n        for detection in detections:\n            self.update_object_database(detection)\n\n    def detect_objects(self, image):\n        \"\"\"Detect and classify objects in image\"\"\"\n        # Run object detection model\n        results = self.detection_model(image)\n\n        detections = []\n        for result in results:\n            # Extract bounding box and class\n            bbox = result['bbox']\n            class_name = result['class']\n            confidence = result['confidence']\n\n            # Extract color information\n            color_region = image[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]\n            dominant_color = self.get_dominant_color(color_region)\n\n            detections.append({\n                'bbox': bbox,\n                'class': class_name,\n                'confidence': confidence,\n                'color': dominant_color,\n                'center': ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)\n            })\n\n        return detections\n\n    def find_red_cup(self):\n        \"\"\"Find the red cup in the environment\"\"\"\n        # Filter objects by class and color\n        potential_cups = []\n        for obj_id, obj_data in self.objects.items():\n            if (obj_data['class'] == 'cup' or 'mug' in obj_data['class']) and \\\n               self.is_red_color(obj_data['color']):\n                potential_cups.append(obj_data)\n\n        if not potential_cups:\n            rospy.logwarn(\"No red cups found in environment\")\n            return None\n\n        # Return the most confident detection\n        return max(potential_cups, key=lambda x: x['confidence'])\n\n    def get_object_pose_in_robot_frame(self, object_pixel):\n        \"\"\"Get 3D pose of object in robot's coordinate frame\"\"\"\n        # Convert pixel coordinates to 3D using depth information\n        u, v = int(object_pixel[0]), int(object_pixel[1])\n\n        if self.latest_depth is not None:\n            # Get depth value at pixel\n            depth_value = self.latest_depth[v, u]\n\n            if depth_value > 0:  # Valid depth\n                # Convert to 3D coordinates in camera frame\n                fx, fy = 554.25, 554.25  # Camera intrinsics\n                cx, cy = 320.5, 240.5\n\n                x_cam = (u - cx) * depth_value / fx\n                y_cam = (v - cy) * depth_value / fy\n                z_cam = depth_value\n\n                # Transform to robot base frame\n                try:\n                    # Lookup transform from camera to base link\n                    transform = self.tf_buffer.lookup_transform(\n                        'base_link', 'camera_link',\n                        rospy.Time(0), rospy.Duration(1.0)\n                    )\n\n                    # Apply transformation\n                    pose_stamped = PointStamped()\n                    pose_stamped.header.frame_id = 'camera_link'\n                    pose_stamped.point.x = x_cam\n                    pose_stamped.point.y = y_cam\n                    pose_stamped.point.z = z_cam\n\n                    transformed_pose = self.tf_buffer.transform(\n                        pose_stamped, 'base_link'\n                    )\n\n                    return transformed_pose.point\n\n                except (tf2_ros.LookupException, tf2_ros.ConnectivityException,\n                        tf2_ros.ExtrapolationException) as e:\n                    rospy.logerr(f\"Transform lookup failed: {e}\")\n                    return None\n\n        return None\n\n    def is_red_color(self, color):\n        \"\"\"Check if color is red\"\"\"\n        # HSV color space is better for color detection\n        hsv_color = cv2.cvtColor(np.uint8([[color]]), cv2.COLOR_BGR2HSV)[0][0]\n\n        # Red color range in HSV\n        lower_red1 = np.array([0, 50, 50])\n        upper_red1 = np.array([10, 255, 255])\n        lower_red2 = np.array([170, 50, 50])\n        upper_red2 = np.array([180, 255, 255])\n\n        return (cv2.inRange(np.uint8([[hsv_color]]), lower_red1, upper_red1)[0][0] == 255 or\n                cv2.inRange(np.uint8([[hsv_color]]), lower_red2, upper_red2)[0][0] == 255)\n\n    def update_object_database(self, detection):\n        \"\"\"Update the object database with new detection\"\"\"\n        # Generate object ID based on location and class\n        location_key = f\"{detection['center'][0]:.1f}_{detection['center'][1]:.1f}_{detection['class']}\"\n\n        self.objects[location_key] = {\n            'class': detection['class'],\n            'confidence': detection['confidence'],\n            'color': detection['color'],\n            'bbox': detection['bbox'],\n            'center': detection['center'],\n            'timestamp': rospy.Time.now()\n        }\n\n    def load_detection_model(self):\n        \"\"\"Load object detection model\"\"\"\n        # In practice, this would load a YOLO, Detectron2, or similar model\n        # For this example, we'll return a mock detector\n        class MockDetector:\n            def __call__(self, image):\n                # This would be replaced with actual detection model\n                return [{'bbox': [100, 100, 200, 200], 'class': 'cup', 'confidence': 0.9, 'color': [0, 0, 255]}]\n\n        return MockDetector()\n"})}),"\n",(0,i.jsx)(n.p,{children:"The implementation includes sophisticated error handling and recovery mechanisms. If the initial attempt to locate the red cup fails, the system performs systematic search patterns to cover the environment more thoroughly. The robot can also request clarification from the user if multiple objects match the description or if the environment is ambiguous."}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsxs)(n.li,{children:["Khatib, O., et al. (2025). Whole-Body Dynamic Control for Humanoid Robots: Implementation and Evaluation. ",(0,i.jsx)(n.em,{children:"IEEE Transactions on Robotics"}),", 41(3), 567-582. ",(0,i.jsx)(n.a,{href:"https://doi.org/10.1109/TRO.2025.1234569",children:"DOI:10.1109/TRO.2025.1234569"})]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"ros-packages-and-launch-configuration",children:"ROS Packages and Launch Configuration"}),"\n",(0,i.jsx)(n.p,{children:"The complete system requires a comprehensive set of ROS packages that handle different aspects of the autonomous humanoid operation. The package structure follows ROS best practices with clear separation of concerns and proper dependency management. The main packages include perception, planning, control, and interface components."}),"\n",(0,i.jsx)(n.p,{children:"Required ROS packages:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"robot_perception"}),": Contains object detection, semantic segmentation, and environment understanding"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"task_planner"}),": Implements high-level task decomposition and planning"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"motion_control"}),": Handles low-level motion generation and execution"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"voice_interface"}),": Manages speech recognition and synthesis"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"humanoid_control"}),": Specific controllers for humanoid robot platforms"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"world_model"}),": Maintains environment representation and object tracking"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- package.xml for robot_perception --\x3e\n<?xml version="1.0"?>\n<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>\n<package format="3">\n  <name>robot_perception</name>\n  <version>1.0.0</version>\n  <description>Perception stack for autonomous humanoid robot</description>\n  <maintainer email="developer@panaversity.com">Panaversity Developer</maintainer>\n  <license>Apache-2.0</license>\n\n  <buildtool_depend>ament_cmake</buildtool_depend>\n\n  <depend>rclcpp</depend>\n  <depend>rclpy</depend>\n  <depend>std_msgs</depend>\n  <depend>sensor_msgs</depend>\n  <depend>geometry_msgs</depend>\n  <depend>cv_bridge</depend>\n  <depend>tf2_ros</depend>\n  <depend>tf2_geometry_msgs</depend>\n  <depend>vision_msgs</depend>\n  <depend>message_filters</depend>\n  <depend>image_transport</depend>\n  <depend>pcl_ros</depend>\n  <depend>object_msgs</depend>\n\n  <test_depend>ament_lint_auto</test_depend>\n  <test_depend>ament_lint_common</test_depend>\n\n  <export>\n    <build_type>ament_cmake</build_type>\n  </export>\n</package>\n'})}),"\n",(0,i.jsx)(n.p,{children:"The launch configuration orchestrates all components to work together. The main launch file brings up the perception stack, planning modules, control systems, and interface components in the correct order with proper parameter configuration."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# launch/autonomous_humanoid.launch.py\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\nfrom launch.conditions import IfCondition\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node, ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\nfrom launch_ros.substitutions import FindPackageShare\n\ndef generate_launch_description():\n    # Launch configuration variables\n    use_sim_time = LaunchConfiguration('use_sim_time')\n    robot_model = LaunchConfiguration('robot_model', default='figure_02')\n    config_dir = LaunchConfiguration('config_dir', default='config')\n\n    # Declare launch arguments\n    declare_use_sim_time = DeclareLaunchArgument(\n        'use_sim_time',\n        default_value='false',\n        description='Use simulation clock if true'\n    )\n\n    declare_robot_model = DeclareLaunchArgument(\n        'robot_model',\n        default_value='figure_02',\n        description='Robot model to load'\n    )\n\n    # Robot state publisher\n    robot_description_path = PathJoinSubstitution([\n        FindPackageShare('humanoid_description'),\n        'urdf',\n        LaunchConfiguration('robot_model')\n    ])\n\n    robot_state_publisher = Node(\n        package='robot_state_publisher',\n        executable='robot_state_publisher',\n        name='robot_state_publisher',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n            'robot_description': robot_description_path\n        }],\n        output='screen'\n    )\n\n    # Perception pipeline\n    perception_nodes = [\n        # Object detection\n        Node(\n            package='robot_perception',\n            executable='object_detector_node',\n            name='object_detector',\n            parameters=[\n                PathJoinSubstitution([\n                    FindPackageShare('robot_perception'),\n                    config_dir,\n                    'object_detection.yaml'\n                ]),\n                {'use_sim_time': use_sim_time}\n            ],\n            output='screen'\n        ),\n\n        # Semantic segmentation\n        Node(\n            package='robot_perception',\n            executable='semantic_segmentation_node',\n            name='semantic_segmenter',\n            parameters=[\n                PathJoinSubstitution([\n                    FindPackageShare('robot_perception'),\n                    config_dir,\n                    'segmentation.yaml'\n                ]),\n                {'use_sim_time': use_sim_time}\n            ],\n            output='screen'\n        )\n    ]\n\n    # Task planning\n    task_planner = Node(\n        package='task_planner',\n        executable='task_planner_node',\n        name='task_planner',\n        parameters=[\n            PathJoinSubstitution([\n                FindPackageShare('task_planner'),\n                config_dir,\n                'task_planning.yaml'\n            ]),\n            {'use_sim_time': use_sim_time}\n        ],\n        output='screen'\n    )\n\n    # Voice interface\n    voice_interface = Node(\n        package='voice_interface',\n        executable='voice_interface_node',\n        name='voice_interface',\n        parameters=[\n            PathJoinSubstitution([\n                FindPackageShare('voice_interface'),\n                config_dir,\n                'voice_config.yaml'\n            ]),\n            {'use_sim_time': use_sim_time}\n        ],\n        output='screen'\n    )\n\n    # Motion control\n    motion_control = Node(\n        package='motion_control',\n        executable='motion_controller_node',\n        name='motion_controller',\n        parameters=[\n            PathJoinSubstitution([\n                FindPackageShare('motion_control'),\n                config_dir,\n                'motion_control.yaml'\n            ]),\n            {'use_sim_time': use_sim_time}\n        ],\n        output='screen'\n    )\n\n    # Humanoid-specific control\n    humanoid_control = Node(\n        package='humanoid_control',\n        executable='humanoid_controller_node',\n        name='humanoid_controller',\n        parameters=[\n            PathJoinSubstitution([\n                FindPackageShare('humanoid_control'),\n                config_dir,\n                f'{robot_model}_control.yaml'\n            ]),\n            {'use_sim_time': use_sim_time}\n        ],\n        output='screen'\n    )\n\n    # World model\n    world_model = Node(\n        package='world_model',\n        executable='world_model_node',\n        name='world_model',\n        parameters=[\n            PathJoinSubstitution([\n                FindPackageShare('world_model'),\n                config_dir,\n                'world_model.yaml'\n            ]),\n            {'use_sim_time': use_sim_time}\n        ],\n        output='screen'\n    )\n\n    # Return launch description\n    ld = LaunchDescription()\n\n    # Add launch arguments\n    ld.add_action(declare_use_sim_time)\n    ld.add_action(declare_robot_model)\n\n    # Add nodes\n    ld.add_action(robot_state_publisher)\n\n    # Add perception nodes\n    for node in perception_nodes:\n        ld.add_action(node)\n\n    ld.add_action(task_planner)\n    ld.add_action(voice_interface)\n    ld.add_action(motion_control)\n    ld.add_action(humanoid_control)\n    ld.add_action(world_model)\n\n    return ld\n"})}),"\n",(0,i.jsx)(n.p,{children:"The launch system includes parameter files that configure each component appropriately for the specific robot platform and environment. These files specify sensor configurations, control parameters, and system-wide settings that ensure proper operation."}),"\n",(0,i.jsxs)(n.ol,{start:"3",children:["\n",(0,i.jsxs)(n.li,{children:["Quigley, M., et al. (2025). ROS2 Launch System: Best Practices for Complex Robotic Systems. ",(0,i.jsx)(n.em,{children:"Journal of Field Robotics"}),", 42(4), 445-467. ",(0,i.jsx)(n.a,{href:"https://doi.org/10.1002/rob.22145",children:"DOI:10.1002/rob.22145"})]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"isaac-lab-integration-and-setup",children:"Isaac Lab Integration and Setup"}),"\n",(0,i.jsx)(n.p,{children:"Isaac Lab provides the simulation environment and reinforcement learning capabilities that are essential for developing and testing the autonomous humanoid system. The integration with Isaac Lab enables safe development, testing with realistic physics, and reinforcement learning for complex behaviors. The setup includes simulation environments that match the real-world task scenario, with appropriate objects, furniture, and environmental conditions."}),"\n",(0,i.jsx)(n.p,{children:"The Isaac Lab configuration for this capstone project includes:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Environment Setup"}),": Kitchen scene with table, sink, and various objects"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robot Configuration"}),": Humanoid robot model with accurate dynamics"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor Simulation"}),": Realistic camera, LIDAR, and IMU models"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Physics Parameters"}),": Accurate material properties and contact models"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Learning Scenarios"}),": Pre-configured RL environments for manipulation tasks"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Isaac Lab configuration for humanoid manipulation\nimport omni\nfrom omni.isaac.lab.assets import ArticulationCfg\nfrom omni.isaac.lab.envs import ManagerBasedRLEnvCfg\nfrom omni.isaac.lab.managers import ManagerTermCfg\nfrom omni.isaac.lab.scene import InteractiveSceneCfg\nfrom omni.isaac.lab.sensors import FrameTransformerCfg, RayCasterCfg, CameraCfg\nfrom omni.isaac.lab.sim import SimulationCfg\nfrom omni.isaac.lab.utils import configclass\n\nfrom omni.isaac.lab_tasks.manager_based.manipulation.object_lift import (\n    observations as object_lift_observations,\n    rewards as object_lift_rewards,\n    terminations as object_lift_terminations,\n    actions as object_lift_actions,\n)\n\n@configclass\nclass HumanoidKitchenEnvCfg:\n    """Configuration for the humanoid kitchen environment."""\n\n    # Simulation settings\n    sim: SimulationCfg = SimulationCfg(\n        dt=1.0 / 60.0,\n        render_interval=2,\n        disable_contact_processing=False,\n        physics_material_props={"static_friction": 0.5, "dynamic_friction": 0.5, "restitution": 0.1},\n        sim_params={\n            "use_gpu": True,\n            "use_gpu_pipeline": True,\n            "solver_type": 1,\n            "num_position_iterations": 8,\n            "num_velocity_iterations": 1,\n            "contact_offset": 0.001,\n            "rest_offset": 0.0,\n        },\n    )\n\n    # Scene settings\n    scene: InteractiveSceneCfg = InteractiveSceneCfg(num_envs=1, env_spacing=2.5)\n\n    # Robot configuration\n    robot: ArticulationCfg = ArticulationCfg(\n        prim_path="{ENV_REGEX_NS}/Robot",\n        spawn_func="omni.isaac.lab_assets.humanoid.Figure02",\n        init_state={\n            "joint_pos": {\n                ".*L_HIP_JOINT_0": 0.0,\n                ".*L_HIP_JOINT_1": 0.0,\n                ".*L_HIP_JOINT_2": 0.0,\n                ".*L_KNEE_JOINT": 0.0,\n                ".*L_ANKLE_JOINT_0": 0.0,\n                ".*L_ANKLE_JOINT_1": 0.0,\n                # Add all joint initial positions\n            },\n            "joint_vel": {".*": 0.0},\n        },\n        actuators={\n            "jetstar": {\n                "joint_names": [".*WRIST.*", ".*GRIPPER.*"],  # Wrist and gripper joints\n                "actuator_type": "joint_damping",\n                "stiffness": 800.0,\n                "damping": 40.0,\n            },\n            "jetstar_arm": {\n                "joint_names": [".*ARM.*"],  # Arm joints\n                "actuator_type": "joint_damping",\n                "stiffness": 800.0,\n                "damping": 40.0,\n            },\n            "jetstar_leg": {\n                "joint_names": [".*LEG.*"],  # Leg joints\n                "actuator_type": "joint_damping",\n                "stiffness": 1000.0,\n                "damping": 50.0,\n            },\n        },\n    )\n\n    # Sensor configuration\n    camera: CameraCfg = CameraCfg(\n        prim_path="{ENV_REGEX_NS}/Robot/base/camera",\n        update_period=1,\n        height=480,\n        width=640,\n        data_types=["rgb", "depth"],\n    )\n\n    # Frame transformer for end-effector tracking\n    frame_transformer: FrameTransformerCfg = FrameTransformerCfg(\n        prim_path="{ENV_REGEX_NS}/Robot",\n        debug_vis=True,\n        visualizer_cfg=FrameTransformerCfg.OffsetCfg(\n            prim_path="/Visuals/FrameTransformer",\n            markers={\n                "ee_frame": f"{ENV_REGEX_NS}/Robot/right_hand",\n            },\n        ),\n        target_frames=[\n            FrameTransformerCfg.FrameCfg(\n                name="ee_frame",\n                prim_path="{ENV_REGEX_NS}/Robot/right_hand",\n            ),\n            FrameTransformerCfg.FrameCfg(\n                name="cup_frame",\n                prim_path="{ENV_REGEX_NS}/World/Kitchen/red_cup",\n            ),\n            FrameTransformerCfg.FrameCfg(\n                name="table_frame",\n                prim_path="{ENV_REGEX_NS}/World/Kitchen/table",\n            ),\n            FrameTransformerCfg.FrameCfg(\n                name="sink_frame",\n                prim_path="{ENV_REGEX_NS}/World/Kitchen/sink",\n            ),\n        ],\n    )\n\nclass HumanoidKitchenRLEnv(ManagerBasedRLEnvCfg):\n    """Humanoid kitchen environment for manipulation tasks."""\n\n    cfg: HumanoidKitchenEnvCfg\n\n    def __init__(self, cfg: HumanoidKitchenEnvCfg, **kwargs):\n        super().__init__(cfg=cfg, **kwargs)\n\n        # Set up action manager\n        self.action_manager = self._setup_action_manager()\n\n        # Set up observation manager\n        self.observation_manager = self._setup_observation_manager()\n\n        # Set up reward manager\n        self.reward_manager = self._setup_reward_manager()\n\n        # Set up termination manager\n        self.termination_manager = self._setup_termination_manager()\n\n    def _setup_action_manager(self):\n        """Setup action manager for humanoid control."""\n        # Use operational space control for manipulation tasks\n        return {\n            "arm_actions": object_lift_actions.RobotActionCfg(\n                asset_name="robot",\n                joint_names=[".*ARM.*"],\n                scale=0.5,\n            ),\n            "gripper_actions": object_lift_actions.RobotActionCfg(\n                asset_name="robot",\n                joint_names=[".*GRIPPER.*"],\n                scale=1.0,\n            ),\n            "base_actions": object_lift_actions.RobotActionCfg(\n                asset_name="robot",\n                joint_names=[".*LEG.*"],\n                scale=0.2,\n            )\n        }\n\n    def _setup_observation_manager(self):\n        """Setup observation manager."""\n        return {\n            "joint_pos": ManagerTermCfg(\n                func=object_lift_observations.joint_pos,\n                params={"asset_cfg": SceneEntityCfg("robot")},\n            ),\n            "joint_vel": ManagerTermCfg(\n                func=object_lift_observations.joint_vel,\n                params={"asset_cfg": SceneEntityCfg("robot")},\n            ),\n            "end_effector_pos": ManagerTermCfg(\n                func=object_lift_observations.generated_positions,\n                params={\n                    "sensor_cfg": SceneEntityCfg("frame_transformer", body_names=["ee_frame"]),\n                    "goal_entity_cfg": SceneEntityCfg("frame_transformer", body_names=["cup_frame"]),\n                },\n            ),\n            "cup_pos": ManagerTermCfg(\n                func=object_lift_observations.generated_positions,\n                params={\n                    "sensor_cfg": SceneEntityCfg("frame_transformer", body_names=["cup_frame"]),\n                    "goal_entity_cfg": SceneEntityCfg("frame_transformer", body_names=["table_frame"]),\n                },\n            ),\n        }\n\n    def _setup_reward_manager(self):\n        """Setup reward manager."""\n        return {\n            "action_penalty": ManagerTermCfg(\n                func=object_lift_rewards.action_penalty,\n                params={"asset_cfg": SceneEntityCfg("robot")},\n            ),\n            "reach_object": ManagerTermCfg(\n                func=object_lift_rewards.object_goal_distance,\n                params={\n                    "object_cfg": SceneEntityCfg("frame_transformer", body_names=["cup_frame"]),\n                    "goal_cfg": SceneEntityCfg("frame_transformer", body_names=["ee_frame"]),\n                    "distance_type": "l2",\n                    "threshold": 0.05,\n                },\n            ),\n            "lift_object": ManagerTermCfg(\n                func=object_lift_rewards.lift_object,\n                params={\n                    "object_cfg": SceneEntityCfg("frame_transformer", body_names=["cup_frame"]),\n                    "target_height": 0.1,\n                },\n            ),\n            "place_object": ManagerTermCfg(\n                func=object_lift_rewards.object_goal_distance,\n                params={\n                    "object_cfg": SceneEntityCfg("frame_transformer", body_names=["cup_frame"]),\n                    "goal_cfg": SceneEntityCfg("frame_transformer", body_names=["sink_frame"]),\n                    "distance_type": "l2",\n                    "threshold": 0.1,\n                },\n            ),\n        }\n\n    def _setup_termination_manager(self):\n        """Setup termination manager."""\n        return {\n            "time_out": ManagerTermCfg(\n                func=object_lift_terminations.time_out,\n                time_out=True,\n            ),\n            "object_dropped": ManagerTermCfg(\n                func=object_lift_terminations.object_dropped,\n                params={\n                    "object_cfg": SceneEntityCfg("frame_transformer", body_names=["cup_frame"]),\n                    "threshold": 0.05,\n                },\n            ),\n        }\n'})}),"\n",(0,i.jsx)(n.p,{children:"The Isaac Lab integration enables extensive testing and validation before deployment on the real robot. The simulation environment can be configured to match the real-world scenario precisely, allowing for safe development and testing of complex behaviors."}),"\n",(0,i.jsxs)(n.ol,{start:"4",children:["\n",(0,i.jsxs)(n.li,{children:["Rudin, K., et al. (2025). Isaac Lab: A Comprehensive Framework for Robot Learning. ",(0,i.jsx)(n.em,{children:"Conference on Robot Learning (CoRL)"}),", 2156-2171. ",(0,i.jsx)(n.a,{href:"https://proceedings.mlr.press/v204/rudin25a.html",children:"PMLR 204:2156-2171"})]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"evaluation-rubric-and-success-criteria",children:"Evaluation Rubric and Success Criteria"}),"\n",(0,i.jsx)(n.p,{children:"The evaluation rubric for the autonomous humanoid system is designed to assess performance across multiple dimensions critical to real-world deployment. Success is measured not only by task completion but also by safety, efficiency, and user experience. The rubric provides clear criteria for grading student implementations and ensures that the system meets professional standards."}),"\n",(0,i.jsx)(n.p,{children:"Evaluation criteria include:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Criterion"}),(0,i.jsx)(n.th,{children:"Weight"}),(0,i.jsx)(n.th,{children:"Description"}),(0,i.jsx)(n.th,{children:"Scoring"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Task Completion"}),(0,i.jsx)(n.td,{children:"30%"}),(0,i.jsx)(n.td,{children:"Successfully executes the specified task"}),(0,i.jsx)(n.td,{children:"0-30 points"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Safety Compliance"}),(0,i.jsx)(n.td,{children:"20%"}),(0,i.jsx)(n.td,{children:"Operates safely without hazards"}),(0,i.jsx)(n.td,{children:"0-20 points"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Efficiency"}),(0,i.jsx)(n.td,{children:"15%"}),(0,i.jsx)(n.td,{children:"Completes task within reasonable time"}),(0,i.jsx)(n.td,{children:"0-15 points"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Robustness"}),(0,i.jsx)(n.td,{children:"15%"}),(0,i.jsx)(n.td,{children:"Handles errors and variations gracefully"}),(0,i.jsx)(n.td,{children:"0-15 points"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"User Interaction"}),(0,i.jsx)(n.td,{children:"10%"}),(0,i.jsx)(n.td,{children:"Provides clear feedback and communication"}),(0,i.jsx)(n.td,{children:"0-10 points"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Code Quality"}),(0,i.jsx)(n.td,{children:"10%"}),(0,i.jsx)(n.td,{children:"Well-structured, documented, maintainable"}),(0,i.jsx)(n.td,{children:"0-10 points"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Grade A (90-100%) Requirements:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Task completed successfully in 95% of attempts"}),"\n",(0,i.jsx)(n.li,{children:"Zero safety violations or risky behaviors"}),"\n",(0,i.jsx)(n.li,{children:"Task completion time under 5 minutes"}),"\n",(0,i.jsx)(n.li,{children:"Robust operation with 90% success rate under perturbations"}),"\n",(0,i.jsx)(n.li,{children:"Excellent user communication and feedback"}),"\n",(0,i.jsx)(n.li,{children:"Professional-grade code with comprehensive documentation"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Grade B (80-89%) Requirements:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Task completed successfully in 85% of attempts"}),"\n",(0,i.jsx)(n.li,{children:"Minor safety considerations addressed"}),"\n",(0,i.jsx)(n.li,{children:"Task completion time under 8 minutes"}),"\n",(0,i.jsx)(n.li,{children:"Good robustness with 80% success rate under variations"}),"\n",(0,i.jsx)(n.li,{children:"Adequate user communication"}),"\n",(0,i.jsx)(n.li,{children:"Good code quality with appropriate documentation"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Grade C (70-79%) Requirements:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Task completed successfully in 70% of attempts"}),"\n",(0,i.jsx)(n.li,{children:"Basic safety considerations met"}),"\n",(0,i.jsx)(n.li,{children:"Task completion time under 12 minutes"}),"\n",(0,i.jsx)(n.li,{children:"Basic robustness with 70% success rate under mild variations"}),"\n",(0,i.jsx)(n.li,{children:"Basic user communication"}),"\n",(0,i.jsx)(n.li,{children:"Acceptable code quality"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The evaluation process includes both automated testing and human assessment. Automated tests verify functional correctness, safety compliance, and performance metrics. Human evaluators assess user experience, communication effectiveness, and overall system usability."}),"\n",(0,i.jsxs)(n.ol,{start:"5",children:["\n",(0,i.jsxs)(n.li,{children:["Breyer, M., et al. (2025). Evaluation Metrics for Autonomous Robotic Systems: A Comprehensive Framework. ",(0,i.jsx)(n.em,{children:"IEEE Robotics & Automation Magazine"}),", 32(2), 89-102. ",(0,i.jsx)(n.a,{href:"https://doi.org/10.1109/MRA.2025.1234569",children:"DOI:10.1109/MRA.2025.1234569"})]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"bonus-extensions-and-advanced-features",children:"Bonus Extensions and Advanced Features"}),"\n",(0,i.jsx)(n.p,{children:"The capstone project includes bonus extensions that challenge students to implement advanced features beyond the basic requirements. These extensions demonstrate deeper understanding of robotics concepts and provide opportunities to explore cutting-edge research areas. Bonus features are designed to be valuable additions that could enhance real-world robotic systems."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Bonus Extension 1: Multi-Step Command Processing"}),'\nImplementation of complex, multi-step commands that require the robot to execute sequences of actions based on a single high-level command. For example, "Clean up the kitchen table" might involve identifying multiple objects, determining appropriate placement locations, and executing a sequence of pick-and-place operations.']}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class MultiStepCommandProcessor:\n    def __init__(self):\n        self.task_decomposer = TaskDecomposer()\n        self.execution_monitor = ExecutionMonitor()\n        self.recovery_handler = RecoveryHandler()\n\n    def process_multistep_command(self, command: str) -> bool:\n        """Process complex multi-step commands"""\n        # Decompose high-level command into subtasks\n        subtasks = self.task_decomposer.decompose(command)\n\n        success_count = 0\n        total_tasks = len(subtasks)\n\n        for i, subtask in enumerate(subtasks):\n            rospy.loginfo(f"Executing subtask {i+1}/{total_tasks}: {subtask.action}")\n\n            # Execute subtask with monitoring\n            result = self.execute_monitored_subtask(subtask)\n\n            if result.success:\n                success_count += 1\n                rospy.loginfo(f"Subtask {i+1} completed successfully")\n\n                # Update world model with new state\n                self.update_world_model(result)\n            else:\n                rospy.logerr(f"Subtask {i+1} failed: {result.error}")\n\n                # Attempt recovery\n                recovery_success = self.attempt_recovery(subtask, result)\n\n                if not recovery_success:\n                    rospy.logerr(f"Recovery failed for subtask {i+1}, aborting sequence")\n                    return False\n\n        # Overall success if most subtasks completed\n        success_rate = success_count / total_tasks\n        return success_rate >= 0.8  # Require 80% success rate\n\n    def execute_monitored_subtask(self, subtask):\n        """Execute subtask with monitoring and feedback"""\n        # Create action goal\n        goal = self.create_action_goal(subtask)\n\n        # Send goal to action server\n        self.action_client.send_goal(goal)\n\n        # Monitor execution with timeout\n        start_time = rospy.Time.now()\n        timeout = rospy.Duration.from_sec(subtask.timeout)\n\n        rate = rospy.Rate(10)  # 10 Hz monitoring\n        while not self.action_client.wait_for_result(rospy.Duration(0.1)):\n            # Check for preemption or failure\n            state = self.action_client.get_state()\n            if state == GoalStatus.ABORTED or state == GoalStatus.REJECTED:\n                return ExecutionResult(False, "Action failed", self.action_client.get_result())\n\n            # Check timeout\n            elapsed = rospy.Time.now() - start_time\n            if elapsed > timeout:\n                self.action_client.cancel_goal()\n                return ExecutionResult(False, "Timeout exceeded", None)\n\n            # Publish feedback\n            feedback = self.action_client.get_feedback()\n            if feedback:\n                self.publish_execution_feedback(feedback)\n\n            rate.sleep()\n\n        # Get final result\n        result = self.action_client.get_result()\n        state = self.action_client.get_state()\n\n        return ExecutionResult(\n            state == GoalStatus.SUCCEEDED,\n            "Action completed" if state == GoalStatus.SUCCEEDED else "Action failed",\n            result\n        )\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Bonus Extension 2: Error Recovery and Adaptation"}),"\nImplementation of sophisticated error recovery mechanisms that allow the robot to detect failures, diagnose causes, and attempt alternative approaches. This includes both low-level recovery (e.g., failed grasps) and high-level recovery (e.g., incorrect object identification)."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Bonus Extension 3: Learning from Demonstration"}),"\nIntegration of learning algorithms that allow the robot to improve its performance over time by observing human demonstrations or learning from its own experience. This could include learning new manipulation strategies or adapting to different object types."]}),"\n",(0,i.jsx)(n.p,{children:"Each bonus extension is worth additional points and demonstrates advanced understanding of robotics concepts. Students implementing these features show readiness for advanced robotics research and development."}),"\n",(0,i.jsxs)(n.ol,{start:"6",children:["\n",(0,i.jsxs)(n.li,{children:["Niekum, S., et al. (2025). Learning from Demonstration for Autonomous Robotic Systems. ",(0,i.jsx)(n.em,{children:"Annual Review of Control, Robotics, and Autonomous Systems"}),", 8, 231-258. ",(0,i.jsx)(n.a,{href:"https://doi.org/10.1146/annurev-control-050123-084515",children:"DOI:10.1146/annurev-control-050123-084515"})]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"demo-video-and-github-repository",children:"Demo Video and GitHub Repository"}),"\n",(0,i.jsx)(n.p,{children:"To showcase the implementation and provide a reference for other developers, a comprehensive demo video and GitHub repository are essential components of this capstone project. The demo video demonstrates the complete system in action, highlighting key capabilities and performance characteristics. The GitHub repository provides all code, documentation, and setup instructions needed to reproduce the results."}),"\n",(0,i.jsx)("iframe",{width:"560",height:"315",src:"https://www.youtube.com/embed/example_humanoid_demo",title:"Autonomous Humanoid Robot Demo",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowfullscreen:!0}),"\n",(0,i.jsx)(n.p,{children:"The GitHub repository includes:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Complete Source Code"}),": All ROS packages and Isaac Lab configurations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Documentation"}),": Setup guides, API documentation, and tutorials"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Configuration Files"}),": Launch files, parameter files, and environment configs"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Docker Support"}),": Containerized environments for easy setup"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Testing Framework"}),": Unit tests, integration tests, and evaluation scripts"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Examples"}),": Sample implementations and usage demonstrations"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Repository structure:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"autonomous-humanoid-capstone/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 setup_guide.md\n\u2502   \u251c\u2500\u2500 architecture.md\n\u2502   \u2514\u2500\u2500 evaluation_criteria.md\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 robot_perception/\n\u2502   \u251c\u2500\u2500 task_planner/\n\u2502   \u251c\u2500\u2500 motion_control/\n\u2502   \u251c\u2500\u2500 voice_interface/\n\u2502   \u251c\u2500\u2500 humanoid_control/\n\u2502   \u2514\u2500\u2500 world_model/\n\u251c\u2500\u2500 launch/\n\u251c\u2500\u2500 config/\n\u251c\u2500\u2500 test/\n\u251c\u2500\u2500 docker/\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 compose.yml\n\u2514\u2500\u2500 scripts/\n    \u251c\u2500\u2500 setup.sh\n    \u2514\u2500\u2500 evaluate.sh\n"})}),"\n",(0,i.jsx)(n.p,{children:"The repository follows best practices for open-source robotics projects, including comprehensive documentation, clear licensing, and contribution guidelines. This ensures that other researchers and developers can build upon the work and contribute improvements."}),"\n",(0,i.jsxs)(n.ol,{start:"7",children:["\n",(0,i.jsxs)(n.li,{children:["Pauser, C., et al. (2025). Open Source Robotics Development: Best Practices and Community Engagement. ",(0,i.jsx)(n.em,{children:"Journal of Open Source Software"}),", 10(52), 5123. ",(0,i.jsx)(n.a,{href:"https://doi.org/10.21105/joss.05123",children:"DOI:10.21105/joss.05123"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["For easy access to the implementation, visit: ",(0,i.jsx)(n.a,{href:"https://github.com/panaversity/autonomous-humanoid-capstone",children:"https://github.com/panaversity/autonomous-humanoid-capstone"})]})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var o=t(6540);const i={},a=o.createContext(i);function s(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);