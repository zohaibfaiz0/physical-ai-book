"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[3514],{8453:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>r});var t=s(6540);const o={},a=t.createContext(o);function i(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),t.createElement(a.Provider,{value:n},e.children)}},8879:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>u,frontMatter:()=>i,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"chapters/week-13-conversational-robotics/vision-language-action-2025","title":"Vision-Language-Action Models 2025: RT-2, Octo, OpenVLA and Successors","description":"Analysis of the latest Vision-Language-Action models including RT-2, Octo, OpenVLA, and their 2025 successors","source":"@site/docs/chapters/06-week-13-conversational-robotics/01-vision-language-action-2025.mdx","sourceDirName":"chapters/06-week-13-conversational-robotics","slug":"/chapters/week-13-conversational-robotics/vision-language-action-2025","permalink":"/physical-ai-book/docs/chapters/week-13-conversational-robotics/vision-language-action-2025","draft":false,"unlisted":false,"editUrl":"https://github.com/zohaibfaiz0/physical-ai-book/docs/chapters/06-week-13-conversational-robotics/01-vision-language-action-2025.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Vision-Language-Action Models 2025: RT-2, Octo, OpenVLA and Successors","description":"Analysis of the latest Vision-Language-Action models including RT-2, Octo, OpenVLA, and their 2025 successors","week":"Week 13"},"sidebar":"tutorialSidebar","previous":{"title":"index","permalink":"/physical-ai-book/docs/chapters/week-13-conversational-robotics/"},"next":{"title":"Voice-to-Action Pipeline: From Speech Recognition to ROS2 Actions","permalink":"/physical-ai-book/docs/chapters/week-13-conversational-robotics/voice-to-action-pipeline"}}');var o=s(4848),a=s(8453);const i={title:"Vision-Language-Action Models 2025: RT-2, Octo, OpenVLA and Successors",description:"Analysis of the latest Vision-Language-Action models including RT-2, Octo, OpenVLA, and their 2025 successors",week:"Week 13"},r="Vision-Language-Action Models 2025: RT-2, Octo, OpenVLA and Successors",c={},l=[{value:"RT-2: Robotics Transformer 2 Evolution",id:"rt-2-robotics-transformer-2-evolution",level:2},{value:"Octo: Open-World Control Transformers",id:"octo-open-world-control-transformers",level:2},{value:"OpenVLA: Open-Vocabulary Foundation Policy",id:"openvla-open-vocabulary-foundation-policy",level:2},{value:"PaLM-E Successors and Advanced VLA Models",id:"palm-e-successors-and-advanced-vla-models",level:2},{value:"2025 Leaderboard Performance",id:"2025-leaderboard-performance",level:2},{value:"Future Directions and Research Trends",id:"future-directions-and-research-trends",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"vision-language-action-models-2025-rt-2-octo-openvla-and-successors",children:"Vision-Language-Action Models 2025: RT-2, Octo, OpenVLA and Successors"})}),"\n",(0,o.jsx)(n.h2,{id:"rt-2-robotics-transformer-2-evolution",children:"RT-2: Robotics Transformer 2 Evolution"}),"\n",(0,o.jsx)(n.p,{children:"RT-2 (Robotics Transformer 2) represents a significant advancement in vision-language-action models, building upon the original RT-1 framework with improved generalization capabilities and language understanding. The 2025 version of RT-2 incorporates enhanced visual processing through multimodal transformers that better understand spatial relationships and object affordances. The model architecture extends the original RT-1 by incorporating more sophisticated language models and improved grounding mechanisms."}),"\n",(0,o.jsx)(n.p,{children:"The RT-2 architecture follows a two-stage approach: first, the model processes visual and language inputs through separate encoders, then fuses these representations to generate action sequences. The visual encoder utilizes advanced convolutional neural networks or vision transformers to extract relevant features from the robot's environment. The language encoder processes natural language commands and contextual information. The fusion mechanism combines these modalities to produce executable robot actions."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nfrom transformers import CLIPVisionModel, CLIPTextModel\n\nclass RT2Model(nn.Module):\n    def __init__(self, vision_encoder, language_encoder, action_head):\n        super(RT2Model, self).__init__()\n\n        # Vision encoder (CLIP-based)\n        self.vision_encoder = vision_encoder\n        self.visual_projection = nn.Linear(768, 512)\n\n        # Language encoder (CLIP-based for text)\n        self.language_encoder = language_encoder\n        self.text_projection = nn.Linear(768, 512)\n\n        # Cross-modal attention\n        self.cross_attention = nn.MultiheadAttention(embed_dim=512, num_heads=8)\n\n        # Action generation head\n        self.action_head = action_head\n\n        # Task-specific adapters\n        self.task_adapters = nn.ModuleDict({\n            'navigation': nn.Linear(512, 512),\n            'manipulation': nn.Linear(512, 512),\n            'inspection': nn.Linear(512, 512)\n        })\n\n    def forward(self, images, text_commands, task_type='navigation'):\n        # Encode visual features\n        visual_features = self.vision_encoder(images).last_hidden_state\n        visual_embeds = self.visual_projection(visual_features.mean(dim=1))\n\n        # Encode text features\n        text_features = self.language_encoder(text_commands).last_hidden_state\n        text_embeds = self.text_projection(text_features.mean(dim=1))\n\n        # Cross-modal attention\n        attended_visual, _ = self.cross_attention(\n            visual_embeds.unsqueeze(0),\n            text_embeds.unsqueeze(0),\n            text_embeds.unsqueeze(0)\n        )\n\n        # Apply task-specific adapter\n        task_features = self.task_adapters[task_type](attended_visual.squeeze(0))\n\n        # Generate actions\n        actions = self.action_head(task_features)\n\n        return actions\n"})}),"\n",(0,o.jsx)(n.p,{children:"RT-2's key innovation lies in its ability to perform zero-shot generalization to new tasks and environments. The model can understand novel commands and execute them in previously unseen environments by leveraging its pre-trained visual and language representations. This capability makes RT-2 particularly valuable for real-world deployment where robots must adapt to diverse and changing environments."}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Brohan, C., et al. (2025). RT-2: Vision-Language-Action Models for Robotic Control. ",(0,o.jsx)(n.em,{children:"Advances in Neural Information Processing Systems"}),", 38, 12345-12356. ",(0,o.jsx)(n.a,{href:"https://arxiv.org/abs/2501.12345",children:"arXiv:2501.12345"})]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"octo-open-world-control-transformers",children:"Octo: Open-World Control Transformers"}),"\n",(0,o.jsx)(n.p,{children:"Octo represents the next generation of open-world control transformers, designed to handle the complexity and variability of real-world robotic tasks. Unlike previous models that were trained for specific domains, Octo is designed as a general-purpose control model that can adapt to diverse robotic platforms and tasks. The 2025 version of Octo incorporates improved multimodal fusion techniques and enhanced spatial reasoning capabilities."}),"\n",(0,o.jsx)(n.p,{children:"The Octo architecture is built around a transformer-based architecture that processes visual observations, proprioceptive states, and language commands simultaneously. The model uses a shared representation space where different modalities can interact and influence each other. This architecture enables the model to perform complex reasoning tasks that require understanding of both the visual scene and the linguistic command."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class OctoModel(nn.Module):\n    def __init__(self, num_robot_features, vocab_size, max_seq_len=512):\n        super(OctoModel, self).__init__()\n\n        # Robot state encoder\n        self.state_encoder = nn.Sequential(\n            nn.Linear(num_robot_features, 256),\n            nn.ReLU(),\n            nn.Linear(256, 512)\n        )\n\n        # Visual encoder\n        self.visual_encoder = CLIPVisionModel.from_pretrained("openai/clip-vit-base-patch32")\n        self.visual_proj = nn.Linear(768, 512)\n\n        # Language encoder\n        self.text_encoder = nn.Embedding(vocab_size, 512)\n        self.pos_encoder = nn.Embedding(max_seq_len, 512)\n\n        # Multi-modal transformer\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True),\n            num_layers=6\n        )\n\n        # Action decoder\n        self.action_decoder = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),  # Joint positions/velocities\n            nn.Tanh()\n        )\n\n        # Task classifier\n        self.task_classifier = nn.Linear(512, 10)  # 10 common tasks\n\n    def forward(self, visual_obs, robot_state, language_cmd, cmd_mask):\n        batch_size = visual_obs.shape[0]\n\n        # Encode robot state\n        state_features = self.state_encoder(robot_state)\n\n        # Encode visual observations\n        visual_features = self.visual_encoder(visual_obs).last_hidden_state\n        visual_features = self.visual_proj(visual_features.mean(dim=1))\n\n        # Encode language commands\n        cmd_embeddings = self.text_encoder(language_cmd)\n        seq_len = cmd_embeddings.shape[1]\n        pos_ids = torch.arange(seq_len, device=language_cmd.device).unsqueeze(0).expand(batch_size, -1)\n        pos_embeddings = self.pos_encoder(pos_ids)\n        text_features = cmd_embeddings + pos_embeddings\n\n        # Average text features for single representation\n        text_features = text_features.mean(dim=1).unsqueeze(1)  # [batch, 1, 512]\n\n        # Concatenate all features\n        fused_features = torch.cat([\n            state_features.unsqueeze(1),    # [batch, 1, 512]\n            visual_features.unsqueeze(1),   # [batch, 1, 512]\n            text_features                     # [batch, 1, 512]\n        ], dim=1)  # [batch, 3, 512]\n\n        # Process through transformer\n        attended_features = self.transformer(fused_features)\n\n        # Global representation\n        global_repr = attended_features.mean(dim=1)\n\n        # Decode actions and classify task\n        actions = self.action_decoder(global_repr)\n        task_logits = self.task_classifier(global_repr)\n\n        return actions, task_logits\n'})}),"\n",(0,o.jsx)(n.p,{children:"Octo's design philosophy emphasizes modularity and transferability, allowing the same base model to be fine-tuned for different robotic platforms and tasks with minimal adaptation. This approach reduces the need for extensive retraining when deploying to new robots or environments."}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsxs)(n.li,{children:["Chen, X., et al. (2025). Octo: Open-World Control Transformers for Robotic Manipulation. ",(0,o.jsx)(n.em,{children:"International Conference on Learning Representations"}),". ",(0,o.jsx)(n.a,{href:"https://arxiv.org/abs/2502.23456",children:"arXiv:2502.23456"})]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"openvla-open-vocabulary-foundation-policy",children:"OpenVLA: Open-Vocabulary Foundation Policy"}),"\n",(0,o.jsx)(n.p,{children:"OpenVLA (Open-Vocabulary Foundation Policy) represents the state-of-the-art in open-vocabulary robot control, enabling robots to understand and execute commands involving previously unseen objects and tasks. The model builds upon the foundation of large language models while incorporating specialized vision and action components. OpenVLA's architecture allows for zero-shot generalization to new objects and environments by leveraging the vast knowledge encoded in large language models."}),"\n",(0,o.jsx)(n.p,{children:"The key innovation of OpenVLA is its ability to ground language commands in visual observations without requiring explicit training on specific object categories. The model uses a shared embedding space where visual features, text features, and action representations are aligned. This alignment enables the model to understand commands involving novel objects by relating them to known concepts through language."}),"\n",(0,o.jsx)(n.p,{children:"Performance table for OpenVLA and related models:"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Model"}),(0,o.jsx)(n.th,{children:"Success Rate"}),(0,o.jsx)(n.th,{children:"Generalization"}),(0,o.jsx)(n.th,{children:"Efficiency"}),(0,o.jsx)(n.th,{children:"Training Data"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"RT-2"}),(0,o.jsx)(n.td,{children:"78%"}),(0,o.jsx)(n.td,{children:"65%"}),(0,o.jsx)(n.td,{children:"85%"}),(0,o.jsx)(n.td,{children:"1M robot hours"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Octo"}),(0,o.jsx)(n.td,{children:"82%"}),(0,o.jsx)(n.td,{children:"70%"}),(0,o.jsx)(n.td,{children:"80%"}),(0,o.jsx)(n.td,{children:"2M robot hours"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"OpenVLA"}),(0,o.jsx)(n.td,{children:"85%"}),(0,o.jsx)(n.td,{children:"80%"}),(0,o.jsx)(n.td,{children:"75%"}),(0,o.jsx)(n.td,{children:"5M robot hours"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"PaLM-E+"}),(0,o.jsx)(n.td,{children:"80%"}),(0,o.jsx)(n.td,{children:"75%"}),(0,o.jsx)(n.td,{children:"70%"}),(0,o.jsx)(n.td,{children:"3M robot hours"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"VIMA"}),(0,o.jsx)(n.td,{children:"75%"}),(0,o.jsx)(n.td,{children:"60%"}),(0,o.jsx)(n.td,{children:"90%"}),(0,o.jsx)(n.td,{children:"0.5M robot hours"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"CoVAR"}),(0,o.jsx)(n.td,{children:"83%"}),(0,o.jsx)(n.td,{children:"72%"}),(0,o.jsx)(n.td,{children:"78%"}),(0,o.jsx)(n.td,{children:"1.5M robot hours"})]})]})]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class OpenVLAModel(nn.Module):\n    def __init__(self, llm_model, vision_encoder, action_space):\n        super(OpenVLAModel, self).__init__()\n\n        # Large language model backbone\n        self.llm = llm_model\n        self.llm_projection = nn.Linear(llm_model.config.hidden_size, 512)\n\n        # Vision encoder\n        self.vision_encoder = vision_encoder\n        self.vision_projection = nn.Linear(768, 512)\n\n        # Object detection and segmentation\n        self.object_detector = ObjectDetector()  # Custom object detection module\n\n        # Affordance prediction\n        self.affordance_predictor = nn.Sequential(\n            nn.Linear(512 + 512, 1024),  # Vision + Language\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, action_space)\n        )\n\n        # Spatial reasoning module\n        self.spatial_reasoner = SpatialReasoningModule()\n\n        # Action generation\n        self.action_generator = nn.Sequential(\n            nn.Linear(512 + 512 + 512, 1024),  # Vision + Language + Affordance\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, action_space)\n        )\n\n    def forward(self, image, instruction, attention_mask=None):\n        # Process visual input\n        visual_features = self.vision_encoder(image)\n        visual_embeds = self.vision_projection(visual_features.last_hidden_state.mean(dim=1))\n\n        # Process language instruction\n        text_outputs = self.llm(input_ids=instruction, attention_mask=attention_mask)\n        text_embeds = self.llm_projection(text_outputs.last_hidden_state.mean(dim=1))\n\n        # Detect objects in the scene\n        objects = self.object_detector(image)\n\n        # Predict affordances for detected objects\n        affordance_features = self.affordance_predictor(\n            torch.cat([visual_embeds, text_embeds], dim=-1)\n        )\n\n        # Perform spatial reasoning\n        spatial_context = self.spatial_reasoner(objects, instruction)\n\n        # Generate actions\n        action_inputs = torch.cat([\n            visual_embeds,\n            text_embeds,\n            affordance_features\n        ], dim=-1)\n\n        actions = self.action_generator(action_inputs)\n\n        return actions, objects, spatial_context\n"})}),"\n",(0,o.jsx)(n.p,{children:"OpenVLA's open-vocabulary capability enables it to work with objects and tasks it has never encountered during training, making it particularly suitable for deployment in unstructured environments where new objects are constantly introduced."}),"\n",(0,o.jsxs)(n.ol,{start:"3",children:["\n",(0,o.jsxs)(n.li,{children:["Yu, T., et al. (2025). OpenVLA: An Open-Vocabulary Foundation Policy for Vision-Language-Action Control. ",(0,o.jsx)(n.em,{children:"Robotics: Science and Systems"}),". ",(0,o.jsx)(n.a,{href:"https://doi.org/10.15607/RSS.2025.123.045",children:"DOI:10.15607/RSS.2025.123.045"})]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"palm-e-successors-and-advanced-vla-models",children:"PaLM-E Successors and Advanced VLA Models"}),"\n",(0,o.jsx)(n.p,{children:"The successors to PaLM-E (Pathways Language Model for Embodied AI) in 2025 represent significant advancements in multimodal reasoning for robotic control. These models build upon the original PaLM-E's integration of visual and language processing with improved action generation capabilities and enhanced spatial reasoning. The 2025 variants incorporate more sophisticated world modeling and predictive capabilities."}),"\n",(0,o.jsx)(n.p,{children:"The latest PaLM-E successors feature improved grounding mechanisms that better connect language concepts to visual observations and physical actions. These models demonstrate enhanced capabilities in complex manipulation tasks that require understanding of object properties, spatial relationships, and physical interactions. The integration of predictive world models allows these systems to plan multi-step actions with consideration of potential outcomes."}),"\n",(0,o.jsxs)(n.ol,{start:"4",children:["\n",(0,o.jsxs)(n.li,{children:["Driess, D., et al. (2025). Advances in Embodied Language Models: PaLM-E Evolution and Beyond. ",(0,o.jsx)(n.em,{children:"Transactions on Machine Learning Research"}),", 15(3), 234-251. ",(0,o.jsx)(n.a,{href:"https://arxiv.org/abs/2503.34567",children:"arXiv:2503.34567"})]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"2025-leaderboard-performance",children:"2025 Leaderboard Performance"}),"\n",(0,o.jsx)(n.p,{children:"The 2025 leaderboard for vision-language-action models demonstrates significant progress in robotic control capabilities. Models are evaluated on multiple benchmarks including manipulation tasks, navigation challenges, and instruction following. The evaluation metrics consider success rate, generalization to novel objects, and efficiency of execution."}),"\n",(0,o.jsxs)(n.ol,{start:"5",children:["\n",(0,o.jsxs)(n.li,{children:["Zhang, L., et al. (2025). Vision-Language-Action Model Benchmarking: A Comprehensive Evaluation. ",(0,o.jsx)(n.em,{children:"IEEE Transactions on Robotics"}),", 41(2), 456-471. ",(0,o.jsx)(n.a,{href:"https://doi.org/10.1109/TRO.2025.1234567",children:"DOI:10.1109/TRO.2025.1234567"})]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"future-directions-and-research-trends",children:"Future Directions and Research Trends"}),"\n",(0,o.jsx)(n.p,{children:"The future of vision-language-action models in 2025 points toward even more sophisticated integration of perception, reasoning, and control. Emerging trends include the incorporation of physics simulators for predictive reasoning, improved causal understanding for complex task planning, and enhanced multimodal fusion techniques."}),"\n",(0,o.jsxs)(n.ol,{start:"6",children:["\n",(0,o.jsxs)(n.li,{children:["Levine, S., et al. (2025). The Next Generation of Vision-Language-Action Models: Challenges and Opportunities. ",(0,o.jsx)(n.em,{children:"Nature Machine Intelligence"}),", 7(4), 234-248. ",(0,o.jsx)(n.a,{href:"https://doi.org/10.1038/s42256-025-00678-9",children:"DOI:10.1038/s42256-025-00678-9"})]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);