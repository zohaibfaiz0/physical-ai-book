"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[6508],{7724:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>m,frontMatter:()=>r,metadata:()=>t,toc:()=>g});const t=JSON.parse('{"id":"chapters/weeks-3-5-ros2/rclpy-agent-bridge","title":"rclpy-agent-bridge","description":"---","source":"@site/docs/chapters/02-weeks-3-5-ros2/04-rclpy-agent-bridge.mdx","sourceDirName":"chapters/02-weeks-3-5-ros2","slug":"/chapters/weeks-3-5-ros2/rclpy-agent-bridge","permalink":"/physical-ai-book/docs/chapters/weeks-3-5-ros2/rclpy-agent-bridge","draft":false,"unlisted":false,"editUrl":"https://github.com/zohaibfaiz0/physical-ai-book/docs/chapters/02-weeks-3-5-ros2/04-rclpy-agent-bridge.mdx","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"urdf-xacro-sdf-humanoids","permalink":"/physical-ai-book/docs/chapters/weeks-3-5-ros2/urdf-xacro-sdf-humanoids"},"next":{"title":"ros2-project-walkthrough","permalink":"/physical-ai-book/docs/chapters/weeks-3-5-ros2/ros2-project-walkthrough"}}');var i=a(4848),s=a(8453),o=a(4588);const r={},l=void 0,c={},g=[{value:"title: &quot;RCLPY and Agent Integration: Python Robotics with AI Agents&quot;\ndescription: &quot;Connecting Python-based ROS 2 nodes with AI agents and decision-making systems&quot;\nweek: &quot;Weeks 3\u20135&quot;",id:"title-rclpy-and-agent-integration-python-robotics-with-ai-agentsdescription-connecting-python-based-ros-2-nodes-with-ai-agents-and-decision-making-systemsweek-weeks-35",level:2},{value:"RCLPY Fundamentals: Python Client Library for ROS 2",id:"rclpy-fundamentals-python-client-library-for-ros-2",level:2},{value:"AI Agent Integration Patterns",id:"ai-agent-integration-patterns",level:2},{value:"Advanced Agent Communication Patterns",id:"advanced-agent-communication-patterns",level:2},{value:"Launch Files and System Configuration",id:"launch-files-and-system-configuration",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",hr:"hr",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(o.A,{}),"\n",(0,i.jsx)(n.h2,{id:"title-rclpy-and-agent-integration-python-robotics-with-ai-agentsdescription-connecting-python-based-ros-2-nodes-with-ai-agents-and-decision-making-systemsweek-weeks-35",children:'title: "RCLPY and Agent Integration: Python Robotics with AI Agents"\ndescription: "Connecting Python-based ROS 2 nodes with AI agents and decision-making systems"\nweek: "Weeks 3\u20135"'}),"\n",(0,i.jsx)(n.h1,{id:"rclpy-and-agent-integration-python-robotics-with-ai-agents",children:"RCLPY and Agent Integration: Python Robotics with AI Agents"}),"\n",(0,i.jsx)(n.h2,{id:"rclpy-fundamentals-python-client-library-for-ros-2",children:"RCLPY Fundamentals: Python Client Library for ROS 2"}),"\n",(0,i.jsx)(n.p,{children:"The Robot Client Library for Python (rclpy) provides the Python interface to ROS 2, enabling developers to create ROS 2 nodes using Python's intuitive syntax while maintaining the performance and reliability of the underlying ROS 2 architecture. RCLPY serves as the Python equivalent to rclcpp, implementing the same core concepts of nodes, publishers, subscribers, services, and actions within Python's object-oriented framework. The library abstracts the complexity of the underlying C client library (rcl) while preserving the full functionality of ROS 2 communication patterns."}),"\n",(0,i.jsxs)(n.p,{children:["RCLPY's architecture follows the same patterns as other ROS 2 client libraries, with nodes serving as the primary container for publishers, subscribers, services, and other ROS 2 entities. The library provides context management through the ",(0,i.jsx)(n.code,{children:"rclpy.init()"})," and ",(0,i.jsx)(n.code,{children:"rclpy.shutdown()"})," functions, which initialize and clean up the ROS 2 client library. The event loop is managed through ",(0,i.jsx)(n.code,{children:"rclpy.spin()"})," and related functions, which process incoming messages and service requests while allowing the node to perform other operations."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import String\nimport numpy as np\n\nclass RobotAgentNode(Node):\n    def __init__(self):\n        super().__init__(\'robot_agent_node\')\n\n        # Initialize publishers\n        self.cmd_vel_publisher = self.create_publisher(\n            Twist,\n            \'/cmd_vel\',\n            10\n        )\n\n        self.status_publisher = self.create_publisher(\n            String,\n            \'/agent_status\',\n            10\n        )\n\n        # Initialize subscribers\n        self.scan_subscription = self.create_subscription(\n            LaserScan,\n            \'/scan\',\n            self.scan_callback,\n            10\n        )\n\n        self.odom_subscription = self.create_subscription(\n            String,  # Simplified for example\n            \'/odom\',\n            self.odom_callback,\n            10\n        )\n\n        # Timer for agent decision making\n        self.agent_timer = self.create_timer(\n            0.1,  # 10 Hz\n            self.agent_decision_loop\n        )\n\n        # Internal state\n        self.scan_data = None\n        self.odom_data = None\n        self.current_goal = None\n        self.agent_state = "IDLE"\n\n        self.get_logger().info("Robot Agent Node initialized")\n\n    def scan_callback(self, msg):\n        """Process laser scan data"""\n        self.scan_data = np.array(msg.ranges)\n        self.get_logger().debug(f"Received scan with {len(self.scan_data)} points")\n\n    def odom_callback(self, msg):\n        """Process odometry data"""\n        self.odom_data = msg.data\n        self.get_logger().debug(f"Received odometry: {self.odom_data}")\n\n    def agent_decision_loop(self):\n        """Main agent decision-making loop"""\n        if self.scan_data is not None:\n            # Perform agent decision making\n            cmd_vel = self.make_navigation_decision()\n            self.cmd_vel_publisher.publish(cmd_vel)\n\n            # Update status\n            status_msg = String()\n            status_msg.data = f"Agent state: {self.agent_state}, Goal: {self.current_goal}"\n            self.status_publisher.publish(status_msg)\n\n    def make_navigation_decision(self):\n        """Make navigation decisions based on sensor data"""\n        cmd = Twist()\n\n        if self.scan_data is not None:\n            # Simple obstacle avoidance\n            min_distance = np.min(self.scan_data)\n\n            if min_distance < 0.5:\n                # Obstacle detected, turn\n                cmd.linear.x = 0.0\n                cmd.angular.z = 0.5\n                self.agent_state = "AVOIDING"\n            else:\n                # Clear path, move forward\n                cmd.linear.x = 0.3\n                cmd.angular.z = 0.0\n                self.agent_state = "MOVING"\n\n        return cmd\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    robot_agent = RobotAgentNode()\n\n    try:\n        rclpy.spin(robot_agent)\n    except KeyboardInterrupt:\n        robot_agent.get_logger().info("Interrupted by user")\n    finally:\n        robot_agent.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.p,{children:"RCLPY's parameter system provides dynamic configuration capabilities similar to other ROS 2 client libraries. Parameters can be declared with type information, default values, and validation callbacks, ensuring that nodes receive appropriate configuration values. The parameter system supports hierarchical namespaces and can be configured through launch files, command-line arguments, or parameter files."}),"\n",(0,i.jsx)(n.p,{children:"The library includes sophisticated QoS (Quality of Service) configuration capabilities that allow fine-tuning of communication behavior. Python developers can specify reliability, durability, deadline, and other QoS policies when creating publishers and subscribers, ensuring that different types of data are transmitted with appropriate reliability and timing characteristics."}),"\n",(0,i.jsx)(n.p,{children:"RCLPY also provides execution management through executors, which control how callbacks are processed. The default single-threaded executor processes callbacks sequentially, while the multi-threaded executor can process multiple callbacks concurrently, improving performance for systems with many callbacks or computationally intensive operations."}),"\n",(0,i.jsx)(n.h2,{id:"ai-agent-integration-patterns",children:"AI Agent Integration Patterns"}),"\n",(0,i.jsx)(n.p,{children:"The integration of AI agents with ROS 2 systems requires careful consideration of communication patterns, timing constraints, and data flow architectures. AI agents typically operate at different temporal scales than traditional ROS 2 nodes, requiring sophisticated synchronization mechanisms to ensure proper coordination between high-frequency sensor/actuator interfaces and lower-frequency decision-making processes."}),"\n",(0,i.jsx)(n.p,{children:"The agent integration pattern typically involves a ROS 2 node that acts as a bridge between the ROS 2 communication infrastructure and the AI agent's decision-making system. This bridge node subscribes to relevant sensor topics, processes the data into the format required by the AI agent, and publishes the agent's decisions to appropriate command topics. The bridge must handle timing differences, data buffering, and state synchronization between the real-time ROS 2 system and the potentially non-real-time AI agent."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import String\nfrom std_msgs.msg import Bool\nimport numpy as np\nimport threading\nimport queue\nimport time\n\nclass AIAgentBridge(Node):\n    def __init__(self):\n        super().__init__(\'ai_agent_bridge\')\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.agent_status_pub = self.create_publisher(String, \'/agent_status\', 10)\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10)\n        self.scan_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.scan_callback, 10)\n        self.goal_sub = self.create_subscription(\n            String, \'/goal\', self.goal_callback, 10)\n\n        # Agent control\n        self.agent_enabled_sub = self.create_subscription(\n            Bool, \'/agent_enabled\', self.agent_control_callback, 10)\n\n        # Internal data structures\n        self.image_queue = queue.Queue(maxsize=5)\n        self.scan_queue = queue.Queue(maxsize=10)\n        self.current_goal = None\n        self.agent_enabled = True\n        self.agent_thread = None\n        self.should_stop = False\n\n        # Start AI agent thread\n        self.agent_thread = threading.Thread(target=self.ai_agent_loop)\n        self.agent_thread.start()\n\n        self.get_logger().info("AI Agent Bridge initialized")\n\n    def image_callback(self, msg):\n        """Handle incoming image data"""\n        if self.agent_enabled and not self.image_queue.full():\n            try:\n                # Convert ROS image to format suitable for AI processing\n                image_data = self.process_ros_image(msg)\n                self.image_queue.put_nowait(image_data)\n            except queue.Full:\n                self.get_logger().warn("Image queue is full, dropping frame")\n\n    def scan_callback(self, msg):\n        """Handle incoming laser scan data"""\n        if self.agent_enabled and not self.scan_queue.full():\n            try:\n                # Process laser scan data\n                scan_data = np.array(msg.ranges)\n                self.scan_queue.put_nowait(scan_data)\n            except queue.Full:\n                self.get_logger().warn("Scan queue is full, dropping data")\n\n    def goal_callback(self, msg):\n        """Handle incoming goal commands"""\n        self.current_goal = msg.data\n        self.get_logger().info(f"New goal received: {self.current_goal}")\n\n    def agent_control_callback(self, msg):\n        """Handle agent enable/disable commands"""\n        self.agent_enabled = msg.data\n        self.get_logger().info(f"Agent enabled: {self.agent_enabled}")\n\n    def process_ros_image(self, ros_image):\n        """Convert ROS image message to numpy array"""\n        # Simplified conversion - in practice, use cv_bridge\n        return np.array(ros_image.data).reshape(\n            ros_image.height, ros_image.width, -1\n        )\n\n    def ai_agent_loop(self):\n        """Main AI agent processing loop"""\n        while not self.should_stop:\n            if self.agent_enabled:\n                # Get latest sensor data\n                latest_scan = None\n                latest_image = None\n\n                # Get latest scan (non-blocking)\n                try:\n                    while not self.scan_queue.empty():\n                        latest_scan = self.scan_queue.get_nowait()\n                except queue.Empty:\n                    pass\n\n                # Get latest image (non-blocking)\n                try:\n                    while not self.image_queue.empty():\n                        latest_image = self.image_queue.get_nowait()\n                except queue.Empty:\n                    pass\n\n                if latest_scan is not None:\n                    # Process with AI agent\n                    action = self.run_ai_agent(latest_scan, latest_image)\n                    if action:\n                        self.cmd_vel_pub.publish(action)\n\n            time.sleep(0.05)  # 20 Hz processing rate\n\n    def run_ai_agent(self, scan_data, image_data):\n        """Execute AI agent decision making"""\n        cmd_vel = Twist()\n\n        # Example AI decision logic\n        if scan_data is not None:\n            min_dist = np.min(scan_data[np.isfinite(scan_data)])\n\n            if min_dist < 0.8:\n                # Obstacle avoidance\n                cmd_vel.linear.x = 0.0\n                cmd_vel.angular.z = 0.8\n            elif self.current_goal:\n                # Navigate toward goal\n                cmd_vel.linear.x = 0.5\n                cmd_vel.angular.z = 0.0\n            else:\n                # Stop if no goal\n                cmd_vel.linear.x = 0.0\n                cmd_vel.angular.z = 0.0\n\n        return cmd_vel\n\n    def destroy_node(self):\n        """Clean up before node destruction"""\n        self.should_stop = True\n        if self.agent_thread:\n            self.agent_thread.join(timeout=2.0)\n        super().destroy_node()\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    ai_bridge = AIAgentBridge()\n\n    try:\n        rclpy.spin(ai_bridge)\n    except KeyboardInterrupt:\n        ai_bridge.get_logger().info("Interrupted by user")\n    finally:\n        ai_bridge.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.p,{children:"The agent integration pattern must also handle error conditions gracefully, including sensor failures, agent failures, and communication timeouts. Robust implementations include fallback behaviors and graceful degradation when components fail, ensuring that the robot can maintain safe operation even when the AI agent is not functioning properly."}),"\n",(0,i.jsx)(n.h2,{id:"advanced-agent-communication-patterns",children:"Advanced Agent Communication Patterns"}),"\n",(0,i.jsx)(n.p,{children:"Advanced AI agent integration patterns in ROS 2 include multi-agent systems, hierarchical decision-making architectures, and reinforcement learning integration. Multi-agent systems involve multiple AI agents coordinating to achieve complex tasks, requiring sophisticated communication protocols and coordination mechanisms. These systems often use service calls, actions, or custom topics to coordinate between different agents with specialized capabilities."}),"\n",(0,i.jsx)(n.p,{children:"Hierarchical agent architectures separate high-level planning from low-level execution, with different agents responsible for different temporal and spatial scales of decision making. High-level agents might plan routes or tasks, while low-level agents handle immediate obstacle avoidance and motion control. The communication between these levels must preserve the intent of high-level decisions while allowing low-level adaptation to local conditions."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\nfrom geometry_msgs.msg import PoseStamped\nfrom nav2_msgs.action import NavigateToPose\nfrom std_msgs.msg import String\nfrom std_msgs.msg import Bool\n\nclass HierarchicalAgentNode(Node):\n    def __init__(self):\n        super().__init__(\'hierarchical_agent\')\n\n        # Publishers\n        self.status_pub = self.create_publisher(String, \'/hierarchical_agent_status\', 10)\n        self.high_level_goal_pub = self.create_publisher(String, \'/high_level_goal\', 10)\n\n        # Subscribers\n        self.task_sub = self.create_subscription(\n            String, \'/mission_task\', self.task_callback, 10)\n        self.environment_sub = self.create_subscription(\n            String, \'/environment_status\', self.environment_callback, 10)\n\n        # Action clients for navigation\n        self.nav_client = ActionClient(\n            self,\n            NavigateToPose,\n            \'navigate_to_pose\'\n        )\n\n        # Internal state\n        self.current_mission = None\n        self.current_subgoal = None\n        self.high_level_agent_active = True\n        self.low_level_agent_active = True\n        self.environment_state = "UNKNOWN"\n\n        # Mission timer\n        self.mission_timer = self.create_timer(1.0, self.mission_loop)\n\n        self.get_logger().info("Hierarchical Agent Node initialized")\n\n    def task_callback(self, msg):\n        """Handle incoming mission tasks"""\n        self.current_mission = msg.data\n        self.get_logger().info(f"Received mission: {self.current_mission}")\n        self.execute_high_level_plan()\n\n    def environment_callback(self, msg):\n        """Handle environment updates"""\n        self.environment_state = msg.data\n        self.get_logger().info(f"Environment state: {self.environment_state}")\n\n    def mission_loop(self):\n        """Main mission execution loop"""\n        if self.current_mission:\n            status_msg = String()\n            status_msg.data = f"Mission: {self.current_mission}, State: {self.environment_state}"\n            self.status_pub.publish(status_msg)\n\n    def execute_high_level_plan(self):\n        """Execute high-level mission planning"""\n        if not self.high_level_agent_active:\n            return\n\n        # Example: Parse mission and generate subgoals\n        if self.current_mission == "EXPLORE_ROOM":\n            self.generate_exploration_subgoals()\n        elif self.current_mission == "FOLLOW_ROUTE":\n            self.generate_navigation_subgoals()\n\n        # Execute first subgoal\n        if self.current_subgoal:\n            self.execute_navigation_subgoal()\n\n    def generate_exploration_subgoals(self):\n        """Generate subgoals for room exploration"""\n        # In a real implementation, this would use path planning\n        # algorithms and environment maps to generate exploration points\n        exploration_points = [\n            (1.0, 1.0),\n            (2.0, 1.0),\n            (2.0, 2.0),\n            (1.0, 2.0),\n            (0.0, 1.0)\n        ]\n\n        self.get_logger().info(f"Generated {len(exploration_points)} exploration points")\n        # For this example, just use the first point\n        self.current_subgoal = exploration_points[0]\n\n    def generate_navigation_subgoals(self):\n        """Generate subgoals for route following"""\n        # Parse route and generate navigation goals\n        self.current_subgoal = (5.0, 5.0)  # Example destination\n\n    def execute_navigation_subgoal(self):\n        """Execute a navigation subgoal using Nav2"""\n        if not self.nav_client.wait_for_server(timeout_sec=1.0):\n            self.get_logger().error("Navigation server not available")\n            return\n\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.header.frame_id = \'map\'\n        goal_msg.pose.pose.position.x = self.current_subgoal[0]\n        goal_msg.pose.pose.position.y = self.current_subgoal[1]\n        goal_msg.pose.pose.position.z = 0.0\n        goal_msg.pose.pose.orientation.w = 1.0  # No rotation\n\n        self.get_logger().info(f"Sending navigation goal to ({self.current_subgoal[0]}, {self.current_subgoal[1]})")\n\n        future = self.nav_client.send_goal_async(goal_msg)\n        future.add_done_callback(self.navigation_goal_callback)\n\n    def navigation_goal_callback(self, future):\n        """Handle navigation goal completion"""\n        goal_handle = future.result()\n        if not goal_handle.accepted:\n            self.get_logger().error("Navigation goal was rejected")\n            return\n\n        self.get_logger().info("Navigation goal accepted, waiting for result")\n\n        result_future = goal_handle.get_result_async()\n        result_future.add_done_callback(self.navigation_result_callback)\n\n    def navigation_result_callback(self, future):\n        """Handle navigation result"""\n        result = future.result().result\n        self.get_logger().info(f"Navigation completed with result: {result}")\n\n        # Process next subgoal or mission completion\n        if self.current_mission == "EXPLORE_ROOM":\n            # In a real implementation, continue to next exploration point\n            self.get_logger().info("Exploration subgoal completed")\n        elif self.current_mission == "FOLLOW_ROUTE":\n            self.get_logger().info("Route following completed")\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    hierarchical_agent = HierarchicalAgentNode()\n\n    try:\n        rclpy.spin(hierarchical_agent)\n    except KeyboardInterrupt:\n        hierarchical_agent.get_logger().info("Interrupted by user")\n    finally:\n        hierarchical_agent.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.p,{children:"Reinforcement learning integration patterns involve training agents that can learn from interaction with the environment. These systems often use custom message types to exchange reward signals, state observations, and action selections between the learning algorithm and the robot's control system. The integration must handle the asynchronous nature of learning, where the agent's policy may be updated periodically based on new experience."}),"\n",(0,i.jsx)(n.h2,{id:"launch-files-and-system-configuration",children:"Launch Files and System Configuration"}),"\n",(0,i.jsx)(n.p,{children:"The deployment of AI agent systems in ROS 2 requires sophisticated launch file configurations that coordinate multiple nodes, parameter servers, and external services. Launch files in ROS 2 (using Python launch files) provide powerful mechanisms for configuring complex systems with conditional startup, parameter passing, and node composition."}),"\n",(0,i.jsx)(n.p,{children:"Launch files for AI agent systems typically include the AI agent bridge nodes, sensor processing nodes, control nodes, and visualization tools. The launch system can also start external services like model servers, database connections, or cloud services that the AI agent may depend on."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, RegisterEventHandler\nfrom launch.conditions import IfCondition\nfrom launch.event_handlers import OnProcessExit\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node, ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\nfrom launch_ros.substitutions import FindPackageShare\n\ndef generate_launch_description():\n    # Launch configuration variables\n    use_sim_time = LaunchConfiguration('use_sim_time')\n    agent_enabled = LaunchConfiguration('agent_enabled', default='true')\n    robot_model = LaunchConfiguration('robot_model', default='simple_humanoid')\n\n    # Declare launch arguments\n    declare_use_sim_time = DeclareLaunchArgument(\n        'use_sim_time',\n        default_value='false',\n        description='Use simulation clock if true'\n    )\n\n    declare_agent_enabled = DeclareLaunchArgument(\n        'agent_enabled',\n        default_value='true',\n        description='Enable AI agent if true'\n    )\n\n    # AI Agent Bridge Node\n    ai_agent_bridge = Node(\n        package='robot_ai',\n        executable='ai_agent_bridge',\n        name='ai_agent_bridge',\n        parameters=[\n            {'use_sim_time': use_sim_time},\n            {'agent_enabled': agent_enabled}\n        ],\n        remappings=[\n            ('/camera/image_raw', '/head_camera/image_raw'),\n            ('/scan', '/laser_scan'),\n            ('/cmd_vel', '/cmd_vel_out')\n        ],\n        output='screen'\n    )\n\n    # Perception processing node\n    perception_node = Node(\n        package='perception_pkg',\n        executable='object_detector',\n        name='object_detector',\n        parameters=[\n            {'use_sim_time': use_sim_time},\n            {'model_path': 'yolov8n.pt'}\n        ],\n        output='screen'\n    )\n\n    # Behavior tree node for high-level decision making\n    behavior_tree_node = Node(\n        package='nav2_bt_navigator',\n        executable='bt_navigator',\n        name='bt_navigator',\n        parameters=[\n            PathJoinSubstitution([\n                FindPackageShare('robot_navigation'),\n                'config',\n                'behavior_tree.xml'\n            ]),\n            {'use_sim_time': use_sim_time}\n        ],\n        output='screen'\n    )\n\n    # RViz2 for visualization\n    rviz_config = PathJoinSubstitution([\n        FindPackageShare('robot_viz'),\n        'rviz',\n        'robot_ai.rviz'\n    ])\n\n    rviz_node = Node(\n        package='rviz2',\n        executable='rviz2',\n        name='rviz2',\n        arguments=['-d', rviz_config],\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen',\n        condition=IfCondition(LaunchConfiguration('rviz', default='true'))\n    )\n\n    # Composable nodes for better performance\n    perception_container = ComposableNodeContainer(\n        name='perception_container',\n        namespace='',\n        package='rclcpp_components',\n        executable='component_container',\n        composable_node_descriptions=[\n            ComposableNode(\n                package='image_proc',\n                plugin='image_proc::RectifyNode',\n                name='image_rectifier',\n                remappings=[('image', '/camera/image_raw'),\n                           ('camera_info', '/camera/camera_info'),\n                           ('image_rect', '/camera/image_rect')]\n            ),\n            ComposableNode(\n                package='depth_image_proc',\n                plugin='depth_image_proc::PointCloudXyzrgbNode',\n                name='pointcloud_xyzrgb',\n                remappings=[('rgb/image_rect_color', '/camera/image_rect'),\n                           ('depth/image_rect', '/camera/depth/image_rect'),\n                           ('points', '/camera/points')]\n            )\n        ],\n        output='screen'\n    )\n\n    # Return launch description\n    ld = LaunchDescription()\n\n    # Add launch arguments\n    ld.add_action(declare_use_sim_time)\n    ld.add_action(declare_agent_enabled)\n\n    # Add nodes\n    ld.add_action(ai_agent_bridge)\n    ld.add_action(perception_node)\n    ld.add_action(behavior_tree_node)\n    ld.add_action(rviz_node)\n    ld.add_action(perception_container)\n\n    return ld\n"})}),"\n",(0,i.jsx)(n.p,{children:"The launch system also supports dynamic reconfiguration of parameters, allowing AI agents to adjust their behavior based on changing conditions or operator commands. Parameter files can specify different configurations for different operating modes, such as indoor vs. outdoor operation, or different mission types. The launch system can also include health monitoring and automatic restart capabilities for critical agent components, ensuring system reliability in long-duration operations."})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);