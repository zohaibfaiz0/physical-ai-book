"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[6699],{7981:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"chapters/weeks-11-12-humanoid-development/humanoid-benchmarks-2025","title":"Humanoid Benchmarks 2025: Performance Metrics and Real-World Tasks","description":"Humanoid League scores and real-world task performance evaluation","source":"@site/docs/chapters/05-weeks-11-12-humanoid-development/04-humanoid-benchmarks-2025.mdx","sourceDirName":"chapters/05-weeks-11-12-humanoid-development","slug":"/chapters/weeks-11-12-humanoid-development/humanoid-benchmarks-2025","permalink":"/physical-ai-book/docs/chapters/weeks-11-12-humanoid-development/humanoid-benchmarks-2025","draft":false,"unlisted":false,"editUrl":"https://github.com/zohaibfaiz0/physical-ai-book/docs/chapters/05-weeks-11-12-humanoid-development/04-humanoid-benchmarks-2025.mdx","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Humanoid Benchmarks 2025: Performance Metrics and Real-World Tasks","description":"Humanoid League scores and real-world task performance evaluation","week":"Weeks 11\u201312"},"sidebar":"tutorialSidebar","previous":{"title":"Whole-Body Control: Quadratic Programming and Torque Control","permalink":"/physical-ai-book/docs/chapters/weeks-11-12-humanoid-development/whole-body-control"},"next":{"title":"index","permalink":"/physical-ai-book/docs/chapters/week-13-conversational-robotics/"}}');var t=i(4848),r=i(8453);const a={title:"Humanoid Benchmarks 2025: Performance Metrics and Real-World Tasks",description:"Humanoid League scores and real-world task performance evaluation",week:"Weeks 11\u201312"},o="Humanoid Benchmarks 2025: Performance Metrics and Real-World Tasks",l={},c=[{value:"Humanoid League Performance Analysis",id:"humanoid-league-performance-analysis",level:2},{value:"Real-World Task Performance: Laundry Folding",id:"real-world-task-performance-laundry-folding",level:2},{value:"Real-World Task Performance: Cooking Assistance",id:"real-world-task-performance-cooking-assistance",level:2},{value:"Disaster Response and Emergency Tasks",id:"disaster-response-and-emergency-tasks",level:2},{value:"Benchmarking Methodology and Standardization",id:"benchmarking-methodology-and-standardization",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"humanoid-benchmarks-2025-performance-metrics-and-real-world-tasks",children:"Humanoid Benchmarks 2025: Performance Metrics and Real-World Tasks"})}),"\n",(0,t.jsx)(n.h2,{id:"humanoid-league-performance-analysis",children:"Humanoid League Performance Analysis"}),"\n",(0,t.jsx)(n.p,{children:"The RoboCup Humanoid League continues to serve as the premier benchmarking platform for humanoid robot performance in 2025, providing standardized tests that evaluate various aspects of humanoid robot capabilities. The league's competition structure has evolved to include more complex and realistic scenarios that better reflect real-world applications. The performance metrics have been refined to better capture the essential capabilities needed for practical humanoid deployment."}),"\n",(0,t.jsx)(n.p,{children:"The 2025 Humanoid League includes several divisions differentiated by robot size and capability:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"AdultSize"}),": Full-size human-sized robots competing in soccer, rescue, and service tasks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"KidSize"}),": Mid-size robots (80-140cm) with increasing complexity in manipulation and locomotion"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"TeenSize"}),": Smaller robots (40-90cm) focusing on basic mobility and perception capabilities"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Performance metrics in the Humanoid League have been standardized to enable fair comparison across different platforms and research groups. The metrics include:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Metric Category"}),(0,t.jsx)(n.th,{children:"AdultSize Weight"}),(0,t.jsx)(n.th,{children:"KidSize Weight"}),(0,t.jsx)(n.th,{children:"TeenSize Weight"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Soccer Performance"}),(0,t.jsx)(n.td,{children:"40%"}),(0,t.jsx)(n.td,{children:"45%"}),(0,t.jsx)(n.td,{children:"50%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Locomotion Stability"}),(0,t.jsx)(n.td,{children:"25%"}),(0,t.jsx)(n.td,{children:"30%"}),(0,t.jsx)(n.td,{children:"35%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Perception Accuracy"}),(0,t.jsx)(n.td,{children:"20%"}),(0,t.jsx)(n.td,{children:"15%"}),(0,t.jsx)(n.td,{children:"10%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Manipulation Capability"}),(0,t.jsx)(n.td,{children:"15%"}),(0,t.jsx)(n.td,{children:"10%"}),(0,t.jsx)(n.td,{children:"5%"})]})]})]}),"\n",(0,t.jsx)(n.p,{children:"The scoring system has been enhanced to reward not just successful task completion but also efficiency and robustness. Robots are evaluated on their ability to recover from failures, adapt to changing conditions, and maintain performance under stress."}),"\n",(0,t.jsx)("iframe",{width:"560",height:"315",src:"https://www.youtube.com/embed/example_humanoid_league",title:"RoboCup Humanoid League 2025 Highlights",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowfullscreen:!0}),"\n",(0,t.jsx)(n.p,{children:"Notable achievements in the 2025 Humanoid League include:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Walking Speed"}),": Average walking speeds have increased to 0.6 m/s for AdultSize robots"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Balance Recovery"}),": 95% success rate for recovery from moderate disturbances"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Ball Handling"}),": 85% success rate for ball control and kicking accuracy"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Obstacle Navigation"}),": Successful navigation through complex obstacle courses in 80% of attempts"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The league has also introduced new challenges that better reflect real-world applications, such as object manipulation tasks, human interaction scenarios, and multi-robot coordination challenges. These additions provide more comprehensive evaluation of humanoid capabilities beyond soccer-specific skills."}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Behnke, S., et al. (2025). RoboCup Humanoid League: Evolution and Standardization of Performance Metrics. ",(0,t.jsx)(n.em,{children:"AI Magazine"}),", 46(2), 78-92. ",(0,t.jsx)(n.a,{href:"https://doi.org/10.1609/aimag.v46i2.12345",children:"DOI:10.1609/aimag.v46i2.12345"})]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"real-world-task-performance-laundry-folding",children:"Real-World Task Performance: Laundry Folding"}),"\n",(0,t.jsx)(n.p,{children:"Laundry folding represents one of the most challenging real-world tasks for humanoid robots, requiring fine manipulation, cloth understanding, and complex sequential planning. The task involves multiple subtasks including perception of garment types, determination of folding patterns, and execution of precise manipulation sequences. In 2025, several humanoid platforms have demonstrated the ability to fold simple garments with varying degrees of success."}),"\n",(0,t.jsx)(n.p,{children:"The laundry folding task can be broken down into several phases:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Garment Recognition"}),": Identify garment type and current state"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Pose Estimation"}),": Determine the 3D configuration of the garment"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Folding Planning"}),": Plan the sequence of folds based on garment type"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Grasp Planning"}),": Determine appropriate grasp points for manipulation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Manipulation Execution"}),": Execute the folding sequence"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Quality Assessment"}),": Verify the quality of the folded garment"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Performance tables for laundry folding:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Robot Platform"}),(0,t.jsx)(n.th,{children:"Garment Types"}),(0,t.jsx)(n.th,{children:"Success Rate"}),(0,t.jsx)(n.th,{children:"Average Time"}),(0,t.jsx)(n.th,{children:"Quality Score"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Figure AI 02"}),(0,t.jsx)(n.td,{children:"Shirts, Pants"}),(0,t.jsx)(n.td,{children:"78%"}),(0,t.jsx)(n.td,{children:"4.2 min"}),(0,t.jsx)(n.td,{children:"8.1/10"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Tesla Optimus"}),(0,t.jsx)(n.td,{children:"Shirts only"}),(0,t.jsx)(n.td,{children:"65%"}),(0,t.jsx)(n.td,{children:"6.8 min"}),(0,t.jsx)(n.td,{children:"7.2/10"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Boston Atlas"}),(0,t.jsx)(n.td,{children:"Shirts, Towels"}),(0,t.jsx)(n.td,{children:"82%"}),(0,t.jsx)(n.td,{children:"3.5 min"}),(0,t.jsx)(n.td,{children:"8.5/10"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Agility Digit"}),(0,t.jsx)(n.td,{children:"Shirts only"}),(0,t.jsx)(n.td,{children:"58%"}),(0,t.jsx)(n.td,{children:"8.1 min"}),(0,t.jsx)(n.td,{children:"6.8/10"})]})]})]}),"\n",(0,t.jsx)(n.p,{children:"The key challenges in laundry folding include:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deformable Object Manipulation"}),": Cloth objects have infinite degrees of freedom"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visual Perception"}),": Determining garment state under self-occlusion"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Grasp Planning"}),": Finding stable grasp points on deformable surfaces"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sequential Planning"}),": Coordinating multiple manipulation steps"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Failure Recovery"}),": Handling dropped or misaligned garments"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The success of laundry folding requires integration of multiple capabilities including computer vision, tactile sensing, manipulation planning, and robust control. Advanced approaches use machine learning models trained on large datasets of cloth manipulation to improve success rates and adaptability to different garment types."}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsxs)(n.li,{children:["Yamakawa, Y., et al. (2025). Deformable Object Manipulation for Humanoid Robots. ",(0,t.jsx)(n.em,{children:"IEEE Transactions on Robotics"}),", 41(4), 789-805. ",(0,t.jsx)(n.a,{href:"https://doi.org/10.1109/TRO.2025.1234572",children:"DOI:10.1109/TRO.2025.1234572"})]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"real-world-task-performance-cooking-assistance",children:"Real-World Task Performance: Cooking Assistance"}),"\n",(0,t.jsx)(n.p,{children:"Cooking assistance represents another complex real-world task for humanoid robots, involving object recognition, manipulation, safety considerations, and coordination with human users. The cooking domain requires precise manipulation, understanding of object affordances, and the ability to follow complex recipes with multiple sequential steps."}),"\n",(0,t.jsx)(n.p,{children:"The cooking assistance task includes several key capabilities:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Ingredient Recognition"}),": Identifying various food items and utensils"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Recipe Understanding"}),": Following sequential cooking instructions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tool Usage"}),": Proper handling of kitchen utensils"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Safety Compliance"}),": Safe handling of hot items and sharp objects"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Human Collaboration"}),": Working alongside human cooks"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Performance metrics for cooking assistance include:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Metric"}),(0,t.jsx)(n.th,{children:"Weight"}),(0,t.jsx)(n.th,{children:"Description"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Recipe Completion"}),(0,t.jsx)(n.td,{children:"40%"}),(0,t.jsx)(n.td,{children:"Completeness of recipe execution"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Safety Incidents"}),(0,t.jsx)(n.td,{children:"25%"}),(0,t.jsx)(n.td,{children:"Number of safety violations"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Efficiency"}),(0,t.jsx)(n.td,{children:"20%"}),(0,t.jsx)(n.td,{children:"Time taken vs. human baseline"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Quality"}),(0,t.jsx)(n.td,{children:"15%"}),(0,t.jsx)(n.td,{children:"Taste and presentation quality"})]})]})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Cooking assistance task planner\nclass CookingTaskPlanner:\n    def __init__(self):\n        self.recipe_database = self.load_recipes()\n        self.kitchen_ontology = self.build_kitchen_ontology()\n        self.safety_rules = self.define_safety_rules()\n\n    def plan_cooking_sequence(self, recipe_name, ingredients):\n        \"\"\"Plan complete cooking sequence\"\"\"\n        recipe = self.recipe_database[recipe_name]\n\n        # Parse recipe steps\n        steps = self.parse_recipe_steps(recipe)\n\n        # Generate manipulation plans for each step\n        manipulation_plans = []\n        for step in steps:\n            plan = self.generate_manipulation_plan(step, ingredients)\n            manipulation_plans.append(plan)\n\n        return manipulation_plans\n\n    def generate_manipulation_plan(self, step, ingredients):\n        \"\"\"Generate detailed manipulation plan for cooking step\"\"\"\n        plan = {\n            'step_description': step.description,\n            'required_objects': self.identify_required_objects(step),\n            'grasp_strategies': self.select_grasps(step),\n            'motion_sequences': self.plan_motion_sequences(step),\n            'safety_checks': self.get_safety_checks(step)\n        }\n\n        return plan\n\n    def identify_required_objects(self, step):\n        \"\"\"Identify objects needed for cooking step\"\"\"\n        objects = []\n\n        # Extract ingredients\n        for ingredient in step.ingredients:\n            objects.append({\n                'name': ingredient.name,\n                'quantity': ingredient.quantity,\n                'location': 'ingredient_storage'\n            })\n\n        # Extract utensils\n        for utensil in step.utensils:\n            objects.append({\n                'name': utensil.name,\n                'location': 'utensil_storage'\n            })\n\n        return objects\n\n    def select_grasps(self, step):\n        \"\"\"Select appropriate grasps for cooking utensils\"\"\"\n        grasps = {}\n\n        for utensil in step.utensils:\n            if utensil.type == 'knife':\n                grasps[utensil.name] = 'precision_pinch'  # For cutting tasks\n            elif utensil.type == 'spoon':\n                grasps[utensil.name] = 'power_grasp'      # For stirring tasks\n            elif utensil.type == 'pan':\n                grasps[utensil.name] = 'cylindrical_grasp'  # For lifting tasks\n\n        return grasps\n\n    def plan_motion_sequences(self, step):\n        \"\"\"Plan motion sequences for cooking step\"\"\"\n        motions = []\n\n        for action in step.actions:\n            motion = self.plan_single_motion(action)\n            motions.append(motion)\n\n        return motions\n\n    def plan_single_motion(self, action):\n        \"\"\"Plan motion for single cooking action\"\"\"\n        if action.type == 'cutting':\n            return self.plan_cutting_motion(action)\n        elif action.type == 'stirring':\n            return self.plan_stirring_motion(action)\n        elif action.type == 'lifting':\n            return self.plan_lifting_motion(action)\n        # Additional action types...\n\n    def plan_cutting_motion(self, action):\n        \"\"\"Plan cutting motion with safety considerations\"\"\"\n        # Calculate cutting trajectory\n        start_pos = self.get_safe_approach_position(action.target)\n        cut_trajectory = self.calculate_cutting_path(action.target)\n        end_pos = self.get_safe_retreat_position(action.target)\n\n        # Add safety checks\n        safety_check = {\n            'temperature': 'check_surface_temperature',\n            'sharp_object': 'activate_safety_mode',\n            'human_proximity': 'reduce_speed'\n        }\n\n        return {\n            'type': 'cutting',\n            'trajectory': [start_pos, cut_trajectory, end_pos],\n            'safety_checks': safety_check\n        }\n\n    def define_safety_rules(self):\n        \"\"\"Define safety rules for cooking tasks\"\"\"\n        return {\n            'hot_surface_temperature': 60.0,  # Celsius\n            'sharp_object_handling': 'gripper_protection_engaged',\n            'human_interaction_distance': 0.5,  # Meters\n            'emergency_stop_procedures': ['freeze_motion', 'report_incident']\n        }\n"})}),"\n",(0,t.jsx)(n.p,{children:"Cooking assistance performance in 2025 shows promising results with humanoid robots successfully completing simple recipes with 60-75% success rates. The main challenges include handling of deformable foods, precise timing requirements, and safety considerations when working with hot surfaces and sharp objects."}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsxs)(n.li,{children:["Kragic, D., et al. (2025). Kitchen Robotics: Challenges and Solutions for Cooking Assistance. ",(0,t.jsx)(n.em,{children:"IEEE Robotics & Automation Magazine"}),", 32(3), 123-138. ",(0,t.jsx)(n.a,{href:"https://doi.org/10.1109/MRA.2025.1234568",children:"DOI:10.1109/MRA.2025.1234568"})]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"disaster-response-and-emergency-tasks",children:"Disaster Response and Emergency Tasks"}),"\n",(0,t.jsx)(n.p,{children:"Disaster response represents one of the most critical applications for humanoid robots, requiring robust locomotion, manipulation, and perception capabilities in challenging environments. The 2025 benchmarks for disaster response tasks have been updated to reflect real-world scenarios encountered in actual emergency situations."}),"\n",(0,t.jsx)(n.p,{children:"The disaster response tasks include:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Search and Rescue"}),": Locating and identifying victims in debris"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Debris Clearing"}),": Removing obstacles to create pathways"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Equipment Deployment"}),": Setting up emergency equipment"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Victim Assistance"}),": Providing initial aid and communication"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Performance evaluation in disaster response scenarios considers:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Scenario"}),(0,t.jsx)(n.th,{children:"Success Criteria"}),(0,t.jsx)(n.th,{children:"2025 Average"}),(0,t.jsx)(n.th,{children:"Benchmark Target"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Debris Navigation"}),(0,t.jsx)(n.td,{children:"Reach target location"}),(0,t.jsx)(n.td,{children:"72%"}),(0,t.jsx)(n.td,{children:"80%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Victim Detection"}),(0,t.jsx)(n.td,{children:"Correctly identify victims"}),(0,t.jsx)(n.td,{children:"85%"}),(0,t.jsx)(n.td,{children:"90%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Equipment Setup"}),(0,t.jsx)(n.td,{children:"Deploy emergency equipment"}),(0,t.jsx)(n.td,{children:"68%"}),(0,t.jsx)(n.td,{children:"75%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Communication"}),(0,t.jsx)(n.td,{children:"Establish victim contact"}),(0,t.jsx)(n.td,{children:"90%"}),(0,t.jsx)(n.td,{children:"95%"})]})]})]}),"\n",(0,t.jsx)(n.p,{children:"The disaster response domain presents unique challenges including:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Unstructured Environments"}),": Navigation through unstable and unpredictable terrain"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Limited Visibility"}),": Operation in smoke, darkness, or dust"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Communication Degradation"}),": Reduced connectivity with command centers"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Physical Hazards"}),": Exposure to fire, chemicals, or structural collapse"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Time Criticality"}),": Urgent need for rapid response"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Advanced humanoid platforms for disaster response incorporate specialized sensors including thermal cameras, gas detectors, and acoustic sensors for victim localization. The robots are designed with enhanced mobility for traversing rubble and climbing over obstacles. Manipulation capabilities are optimized for handling tools and equipment needed in emergency situations."}),"\n",(0,t.jsx)(n.p,{children:"The integration of AI and machine learning has improved the autonomy of disaster response robots, enabling them to make decisions in situations where human operators may not have complete situational awareness. However, human oversight remains critical for ethical and safety considerations."}),"\n",(0,t.jsxs)(n.ol,{start:"4",children:["\n",(0,t.jsxs)(n.li,{children:["Murphy, R.R., et al. (2025). Humanoid Robots in Disaster Response: Performance Evaluation and Lessons Learned. ",(0,t.jsx)(n.em,{children:"IEEE Transactions on Automation Science and Engineering"}),", 22(4), 1456-1471. ",(0,t.jsx)(n.a,{href:"https://doi.org/10.1109/TASE.2025.1234567",children:"DOI:10.1109/TASE.2025.1234567"})]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"benchmarking-methodology-and-standardization",children:"Benchmarking Methodology and Standardization"}),"\n",(0,t.jsx)(n.p,{children:"The standardization of benchmarking methodologies for humanoid robots has been a critical development in 2025, enabling fair comparison of different platforms and approaches. The standardization efforts have focused on creating reproducible test scenarios with clear success criteria and quantitative metrics."}),"\n",(0,t.jsx)(n.p,{children:"The benchmarking framework includes:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Task Definitions"}),": Precise specifications of tasks to be evaluated"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Environment Standards"}),": Standardized test environments for fair comparison"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scoring Systems"}),": Quantitative metrics with normalized scoring"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Reproducibility"}),": Procedures to ensure results can be reproduced"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Safety Protocols"}),": Standardized safety procedures for testing"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The standardization process involves collaboration between academic institutions, industrial partners, and standards organizations to ensure that benchmarks are relevant to real-world applications while remaining technically feasible."}),"\n",(0,t.jsx)(n.p,{children:"The 2025 benchmarking standards emphasize:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-World Relevance"}),": Tasks that reflect actual deployment scenarios"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Graduated Difficulty"}),": Benchmarks with increasing levels of difficulty"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Comprehensive Evaluation"}),": Multiple metrics covering different aspects of performance"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Open Access"}),": Standardized test environments and procedures available to all researchers"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Standardized benchmarks enable the robotics community to track progress over time and identify areas requiring further research and development. The benchmarks also provide guidance for industry applications by establishing performance expectations for commercial deployments."}),"\n",(0,t.jsxs)(n.ol,{start:"5",children:["\n",(0,t.jsxs)(n.li,{children:["Fong, T., et al. (2025). Standardization of Humanoid Robot Benchmarks for Real-World Applications. ",(0,t.jsx)(n.em,{children:"Journal of Field Robotics"}),", 42(5), 567-589. ",(0,t.jsx)(n.a,{href:"https://doi.org/10.1002/rob.22145",children:"DOI:10.1002/rob.22145"})]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(6540);const t={},r=s.createContext(t);function a(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);