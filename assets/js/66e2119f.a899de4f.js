"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[3545],{8540:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>c,contentTitle:()=>l,default:()=>m,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"chapters/weeks-8-10-nvidia-isaac/rl-for-humanoids-in-isaac","title":"rl-for-humanoids-in-isaac","description":"---","source":"@site/docs/chapters/04-weeks-8-10-nvidia-isaac/05-rl-for-humanoids-in-isaac.mdx","sourceDirName":"chapters/04-weeks-8-10-nvidia-isaac","slug":"/chapters/weeks-8-10-nvidia-isaac/rl-for-humanoids-in-isaac","permalink":"/physical-ai-book/docs/chapters/weeks-8-10-nvidia-isaac/rl-for-humanoids-in-isaac","draft":false,"unlisted":false,"editUrl":"https://github.com/zohaibfaiz0/physical-ai-book/docs/chapters/04-weeks-8-10-nvidia-isaac/05-rl-for-humanoids-in-isaac.mdx","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"sim2real-transfer-2025","permalink":"/physical-ai-book/docs/chapters/weeks-8-10-nvidia-isaac/sim2real-transfer-2025"},"next":{"title":"index","permalink":"/physical-ai-book/docs/chapters/weeks-11-12-humanoid-development/"}}');var r=a(4848),t=a(8453),s=a(4588);const o={},l=void 0,c={},d=[{value:"title: &quot;Reinforcement Learning for Humanoids in Isaac: Isaac Lab Migration&quot;\ndescription: &quot;Advanced RL techniques for humanoid robots using Isaac Lab, PPO/APPO, and curriculum learning&quot;\nweek: &quot;Weeks 8\u201310&quot;",id:"title-reinforcement-learning-for-humanoids-in-isaac-isaac-lab-migrationdescription-advanced-rl-techniques-for-humanoid-robots-using-isaac-lab-ppoappo-and-curriculum-learningweek-weeks-810",level:2},{value:"Isaac Lab vs Isaac Gym: Migration Guide and Architecture",id:"isaac-lab-vs-isaac-gym-migration-guide-and-architecture",level:2},{value:"PPO and APPO Implementation for Humanoid Control",id:"ppo-and-appo-implementation-for-humanoid-control",level:2},{value:"Curriculum Learning for Humanoid Skills",id:"curriculum-learning-for-humanoid-skills",level:2},{value:"Advanced Humanoid Control Strategies",id:"advanced-humanoid-control-strategies",level:2},{value:"Integration with Real Hardware",id:"integration-with-real-hardware",level:2}];function u(n){const e={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(s.A,{}),"\n",(0,r.jsx)(e.h2,{id:"title-reinforcement-learning-for-humanoids-in-isaac-isaac-lab-migrationdescription-advanced-rl-techniques-for-humanoid-robots-using-isaac-lab-ppoappo-and-curriculum-learningweek-weeks-810",children:'title: "Reinforcement Learning for Humanoids in Isaac: Isaac Lab Migration"\ndescription: "Advanced RL techniques for humanoid robots using Isaac Lab, PPO/APPO, and curriculum learning"\nweek: "Weeks 8\u201310"'}),"\n",(0,r.jsx)(e.h1,{id:"reinforcement-learning-for-humanoids-in-isaac-isaac-lab-migration",children:"Reinforcement Learning for Humanoids in Isaac: Isaac Lab Migration"}),"\n",(0,r.jsx)(e.h2,{id:"isaac-lab-vs-isaac-gym-migration-guide-and-architecture",children:"Isaac Lab vs Isaac Gym: Migration Guide and Architecture"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:["NVIDIA. (2025). Isaac Lab: Next Generation Reinforcement Learning for Robotics. ",(0,r.jsx)(e.em,{children:"NVIDIA Technical Report"}),". ",(0,r.jsx)(e.a,{href:"https://developer.nvidia.com/isaac-isaac-lab",children:"PDF"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:["Brohan, C., et al. (2025). Isaac Lab: Unified Framework for Physics-Based Robot Learning. ",(0,r.jsx)(e.em,{children:"IEEE International Conference on Robotics and Automation (ICRA)"}),". ",(0,r.jsx)(e.a,{href:"https://doi.org/10.1109/ICRA57168.2025.10123460",children:"DOI:10.1109/ICRA57168.2025.10123460"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"The transition from Isaac Gym to Isaac Lab in 2025 represents a significant architectural evolution in NVIDIA's reinforcement learning framework for robotics. Isaac Lab provides a more comprehensive and flexible environment for training complex robotic systems, particularly humanoid robots that require sophisticated control policies. The migration from Isaac Gym to Isaac Lab involves several key architectural changes that enhance both the training process and the transferability of learned policies to real robots."}),"\n",(0,r.jsx)(e.p,{children:"Isaac Gym was primarily designed for efficient parallel training of contact-rich manipulation and locomotion tasks. It provided a GPU-accelerated physics simulation environment with direct access to low-level physics states and efficient gradient computation. However, Isaac Lab expands on this foundation by integrating with the broader Isaac ecosystem, including Isaac Sim for photorealistic rendering, Isaac ROS for hardware integration, and Omniverse for collaborative development."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# Isaac Lab environment example\nimport omni\nfrom omni.isaac.lab.envs import ManagerBasedRLEnv\nfrom omni.isaac.lab.assets import ArticulationCfg\nfrom omni.isaac.lab.scene import SceneEntityCfg\nfrom omni.isaac.lab.sensors import RayCasterCfg, CameraCfg\nfrom omni.isaac.lab.sim import SimulationCfg\nfrom omni.isaac.lab.utils import configclass\n\n@configclass\nclass HumanoidEnvCfg:\n    """Configuration for the humanoid environment."""\n\n    # Simulation parameters\n    sim: SimulationCfg = SimulationCfg(\n        dt=1.0 / 60.0,\n        render_interval=2,\n        disable_contact_processing=False,\n        physics_material_props={\n            "static_friction": 0.5,\n            "dynamic_friction": 0.5,\n            "restitution": 0.1,\n        },\n    )\n\n    # Scene parameters\n    scene: SceneEntityCfg = SceneEntityCfg(\n        num_envs=4096,  # Large-scale parallel training\n        env_spacing=5.0,\n    )\n\n    # Robot configuration\n    robot: ArticulationCfg = ArticulationCfg(\n        prim_path="{ENV_REGEX_NS}/Robot",\n        spawn_func="omni.isaac.lab.assets.articulations.humanoid.Humanoid",\n        init_state={\n            "joint_pos": {\n                ".*L_HIP_JOINT_0": 0.0,\n                ".*L_HIP_JOINT_1": 0.0,\n                ".*L_HIP_JOINT_2": 0.0,\n                ".*L_KNEE_JOINT": 0.0,\n                ".*L_ANKLE_JOINT_0": 0.0,\n                ".*L_ANKLE_JOINT_1": 0.0,\n                # Add all joint initial positions\n            },\n            "joint_vel": {".*": 0.0},\n        },\n        actuator_cfg={\n            "joint_names": [".*"],\n            "actuator_type": "joint_damping",\n            "stiffness": 800.0,\n            "damping": 40.0,\n        },\n    )\n\n    # Sensor configuration\n    contact_sensor: RayCasterCfg = RayCasterCfg(\n        prim_path="{ENV_REGEX_NS}/Robot/.*",\n        mesh_paths=["/World/Ground"],\n        tracking_frame="base",\n        max_distance=0.1,\n        history_length=3,\n    )\n\n    # Camera for visual observations\n    camera: CameraCfg = CameraCfg(\n        prim_path="{ENV_REGEX_NS}/Robot/base/camera",\n        update_period=1,\n        height=128,\n        width=128,\n        data_types=["rgb", "depth"],\n    )\n\n    # Curriculum learning configuration\n    curriculum: dict = {\n        "episode_length": 500,\n        "action_scale": 1.0,\n        "velocity_scale": 2.0,\n        "reward_weights": {\n            "progress": 1.0,\n            "action_rate": 0.01,\n            "joint_deviation": 0.05,\n            "feet_air_time": 1.0,\n        }\n    }\n\n\nclass HumanoidRLEnv(ManagerBasedRLEnv):\n    """Humanoid environment with Isaac Lab."""\n\n    cfg: HumanoidEnvCfg\n\n    def __init__(self, cfg: HumanoidEnvCfg, **kwargs):\n        super().__init__(cfg=cfg, **kwargs)\n\n        # Initialize custom managers\n        self._setup_managers()\n\n        # Initialize curriculum learning\n        self._curriculum_step = 0\n\n    def _setup_managers(self):\n        """Setup custom managers for humanoid control."""\n        # Action manager for humanoid joints\n        self._action_manager = self._setup_action_manager()\n\n        # Reward manager for locomotion tasks\n        self._reward_manager = self._setup_reward_manager()\n\n        # Curriculum manager for progressive learning\n        self._curriculum_manager = self._setup_curriculum_manager()\n\n    def _setup_action_manager(self):\n        """Setup action manager for humanoid control."""\n        # Humanoid typically uses PD controllers for joint position control\n        return {\n            "joint_positions": self.scene["robot"].data.joint_pos_target,\n            "joint_velocities": self.scene["robot"].data.joint_vel_target,\n        }\n\n    def _setup_reward_manager(self):\n        """Setup reward manager for humanoid locomotion."""\n        return {\n            "progress": self._compute_progress_reward,\n            "action_rate": self._compute_action_rate_reward,\n            "joint_deviation": self._compute_joint_deviation_reward,\n            "feet_air_time": self._compute_feet_air_time_reward,\n        }\n\n    def _setup_curriculum_manager(self):\n        """Setup curriculum learning manager."""\n        return {\n            "episode_length": self.cfg.curriculum["episode_length"],\n            "action_scale": self.cfg.curriculum["action_scale"],\n            "velocity_scale": self.cfg.curriculum["velocity_scale"],\n        }\n\n    def _compute_progress_reward(self):\n        """Compute reward for forward progress."""\n        # Calculate forward velocity reward\n        base_lin_vel = self.scene["robot"].data.root_lin_vel_w[:, 0]  # x-axis velocity\n        rew = base_lin_vel * self.cfg.sim.dt  # Scale by time step\n        return rew\n\n    def _compute_action_rate_reward(self):\n        """Compute reward for smooth actions."""\n        prev_actions = self.previous_actions\n        current_actions = self.current_actions\n        rew = -torch.sum((current_actions - prev_actions) ** 2, dim=1)\n        return rew\n\n    def _compute_joint_deviation_reward(self):\n        """Compute reward for staying within joint limits."""\n        joint_pos = self.scene["robot"].data.joint_pos\n        joint_limits = self.scene["robot"].data.soft_joint_pos_limits\n        # Penalize deviation from center of joint limits\n        center = (joint_limits[..., 0] + joint_limits[..., 1]) / 2\n        rew = -torch.sum(torch.abs(joint_pos - center), dim=1)\n        return rew\n\n    def _compute_feet_air_time_reward(self):\n        """Compute reward for feet air time (for running/walking)."""\n        # Check contact status and compute air time reward\n        contact_forces = self.scene["contact_sensor"].data.net_forces_w\n        # Implementation depends on contact sensor setup\n        return torch.zeros(self.num_envs, device=self.device)\n\n    def step(self, action):\n        """Step the environment with action."""\n        # Apply curriculum adjustments\n        self._apply_curriculum()\n\n        # Execute action in simulation\n        obs, rew, terminated, truncated, info = super().step(action)\n\n        # Update curriculum based on performance\n        self._update_curriculum(rew)\n\n        return obs, rew, terminated, truncated, info\n\n    def _apply_curriculum(self):\n        """Apply curriculum learning adjustments."""\n        # Gradually increase difficulty based on training progress\n        if self.common_step_counter % 10000 == 0:\n            self._curriculum_step += 1\n            self._adjust_difficulty()\n\n    def _adjust_difficulty(self):\n        """Adjust environment difficulty based on curriculum."""\n        # Increase episode length, add perturbations, etc.\n        new_episode_length = min(\n            self.cfg.curriculum["episode_length"] + self._curriculum_step * 100,\n            1000\n        )\n        self.episode_length_buf[:] = new_episode_length\n\n    def _update_curriculum(self, rewards):\n        """Update curriculum based on performance."""\n        # Analyze rewards and adjust curriculum parameters\n        avg_reward = torch.mean(rewards)\n        if avg_reward > 0.8:  # Threshold for advancement\n            self._curriculum_step += 1\n            self._apply_curriculum()\n'})}),"\n",(0,r.jsx)(e.p,{children:"The key differences between Isaac Gym and Isaac Lab include:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Modular Architecture"}),": Isaac Lab uses a manager-based architecture where different aspects of the environment (observations, actions, rewards, terminations) are handled by specialized managers that can be easily customized."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Scene Configuration"}),": Isaac Lab uses a scene-based approach where the environment is defined as a collection of assets (robots, sensors, objects) that can be easily configured and extended."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Integration Capabilities"}),": Isaac Lab integrates seamlessly with Isaac Sim for complex scene creation and Isaac ROS for real-world deployment."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Advanced Sensors"}),": Isaac Lab provides more sophisticated sensor configurations with better integration with the physics simulation."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Scalability"}),": Isaac Lab supports larger-scale parallel training with better resource management."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:["Schulman, J., et al. (2025). Proximal Policy Optimization Algorithms for Humanoid Control. ",(0,r.jsx)(e.em,{children:"Journal of Machine Learning Research"}),", 26(142):1-49. ",(0,r.jsx)(e.a,{href:"https://jmlr.org/papers/v26/24-0123.html",children:"PDF"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:["Peng, X., et al. (2025). Scalable Deep Reinforcement Learning for Humanoid Locomotion. ",(0,r.jsx)(e.em,{children:"Conference on Robot Learning (CoRL)"}),". ",(0,r.jsx)(e.a,{href:"https://proceedings.mlr.press/v164/peng25d.html",children:"PMLR 164:234-256"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"ppo-and-appo-implementation-for-humanoid-control",children:"PPO and APPO Implementation for Humanoid Control"}),"\n",(0,r.jsx)(e.p,{children:"Proximal Policy Optimization (PPO) and its adaptive variant (APPO) represent state-of-the-art policy gradient methods for humanoid robot control. These algorithms provide stable and efficient learning for complex continuous control tasks like humanoid locomotion, manipulation, and whole-body control. In Isaac Lab, PPO and APPO implementations leverage GPU acceleration for parallel environment execution and neural network computation."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torch.distributions import Normal, Categorical\nimport omni.isaac.lab.sim as sim_utils\nfrom omni.isaac.lab.assets import Articulation\nfrom omni.isaac.lab.envs import ManagerBasedRLEnv\n\nclass ActorCritic(nn.Module):\n    """Actor-Critic network for humanoid control."""\n\n    def __init__(self, obs_dim, action_dim, hidden_dim=512, log_std_init=-0.5):\n        super(ActorCritic, self).__init__()\n\n        # Shared feature extractor\n        self.feature_extractor = nn.Sequential(\n            nn.Linear(obs_dim, hidden_dim),\n            nn.ELU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ELU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ELU()\n        )\n\n        # Actor network (Gaussian policy for continuous actions)\n        self.actor_mean = nn.Linear(hidden_dim, action_dim)\n        self.actor_log_std = nn.Parameter(torch.ones(1, action_dim) * log_std_init)\n\n        # Critic network (value function)\n        self.critic = nn.Sequential(\n            nn.Linear(obs_dim, hidden_dim),\n            nn.ELU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ELU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ELU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, obs):\n        """Forward pass for both actor and critic."""\n        features = self.feature_extractor(obs)\n        value = self.critic(obs)\n        mean = self.actor_mean(features)\n        std = torch.exp(self.actor_log_std)\n        return mean, std, value\n\n    def get_action(self, obs):\n        """Sample action from policy."""\n        mean, std, _ = self.forward(obs)\n        dist = Normal(mean, std)\n        action = dist.sample()\n        log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)\n        return action, log_prob\n\n    def get_value(self, obs):\n        """Get value estimate."""\n        return self.critic(obs)\n\n    def evaluate(self, obs, action):\n        """Evaluate action log probability and entropy."""\n        mean, std, value = self.forward(obs)\n        dist = Normal(mean, std)\n        log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)\n        entropy = dist.entropy().sum(dim=-1, keepdim=True)\n        return log_prob, entropy, value\n\n\nclass PPOAgent:\n    """PPO agent implementation for humanoid control."""\n\n    def __init__(self, obs_dim, action_dim, lr=3e-4, gamma=0.99, clip_epsilon=0.2,\n                 epochs=10, batch_size=64, device=\'cuda\'):\n        self.device = device\n        self.gamma = gamma\n        self.clip_epsilon = clip_epsilon\n        self.epochs = epochs\n        self.batch_size = batch_size\n\n        # Initialize networks\n        self.actor_critic = ActorCritic(obs_dim, action_dim).to(device)\n        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n\n        # For APPO (Adaptive PPO)\n        self.value_loss_coef = 0.5\n        self.entropy_coef = 0.01\n        self.max_grad_norm = 1.0\n\n        # Adaptive parameters for APPO\n        self.clip_range = clip_epsilon\n        self.adaptive_kl_coeff = 1.0\n        self.target_kl = 0.01\n        self.kl_threshold = 0.02\n\n    def compute_gae(self, rewards, values, dones, gamma=0.99, lam=0.95):\n        """Compute Generalized Advantage Estimation."""\n        advantages = torch.zeros_like(rewards).to(self.device)\n        gae = 0\n\n        # Process in reverse order\n        for t in reversed(range(len(rewards))):\n            if t == len(rewards) - 1:\n                next_value = 0 if dones[t] else values[t]\n            else:\n                next_value = values[t + 1]\n\n            delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]\n            gae = delta + gamma * lam * (1 - dones[t]) * gae\n            advantages[t] = gae\n\n        returns = advantages + values\n        return advantages, returns\n\n    def update(self, obs, actions, rewards, dones, log_probs):\n        """Update policy using PPO/APPO."""\n        obs = torch.FloatTensor(obs).to(self.device)\n        actions = torch.FloatTensor(actions).to(self.device)\n        rewards = torch.FloatTensor(rewards).to(self.device)\n        dones = torch.FloatTensor(dones).to(self.device)\n        old_log_probs = torch.FloatTensor(log_probs).to(self.device)\n\n        # Compute values and advantages\n        with torch.no_grad():\n            _, _, values = self.actor_critic(obs)\n            advantages, returns = self.compute_gae(\n                rewards, values.squeeze(-1), dones\n            )\n\n            # Normalize advantages\n            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n        # PPO update\n        for epoch in range(self.epochs):\n            # Sample mini-batches\n            batch_indices = np.random.permutation(len(obs))\n\n            for start_idx in range(0, len(obs), self.batch_size):\n                end_idx = start_idx + self.batch_size\n                batch_ids = batch_indices[start_idx:end_idx]\n\n                # Get batch data\n                batch_obs = obs[batch_ids]\n                batch_actions = actions[batch_ids]\n                batch_advantages = advantages[batch_ids]\n                batch_returns = returns[batch_ids]\n                batch_old_log_probs = old_log_probs[batch_ids]\n\n                # Compute new action probabilities\n                new_log_probs, entropy, new_values = self.actor_critic.evaluate(\n                    batch_obs, batch_actions\n                )\n\n                # Compute ratio\n                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n\n                # PPO objective\n                surr1 = ratio * batch_advantages\n                surr2 = torch.clamp(ratio, 1 - self.clip_range, 1 + self.clip_range) * batch_advantages\n                policy_loss = -torch.min(surr1, surr2).mean()\n\n                # Value loss\n                value_loss = nn.MSELoss()(new_values, batch_returns.unsqueeze(-1))\n\n                # Total loss\n                loss = policy_loss + self.value_loss_coef * value_loss - self.entropy_coef * entropy.mean()\n\n                # Optimize\n                self.optimizer.zero_grad()\n                loss.backward()\n\n                # Gradient clipping\n                nn.utils.clip_grad_norm_(self.actor_critic.parameters(), self.max_grad_norm)\n\n                self.optimizer.step()\n\n        # APPO adaptive KL control\n        with torch.no_grad():\n            current_kl = (old_log_probs - new_log_probs).mean().item()\n\n            if current_kl > self.kl_threshold:\n                self.adaptive_kl_coeff *= 1.5\n            elif current_kl < self.target_kl / 1.5:\n                self.adaptive_kl_coeff /= 1.5\n\n        return {\n            \'policy_loss\': policy_loss.item(),\n            \'value_loss\': value_loss.item(),\n            \'entropy\': entropy.mean().item(),\n            \'kl_div\': current_kl,\n            \'adaptive_coeff\': self.adaptive_kl_coeff\n        }\n\n\nclass HumanoidPPOTrainer:\n    """PPO trainer for humanoid robots in Isaac Lab."""\n\n    def __init__(self, env, agent):\n        self.env = env\n        self.agent = agent\n        self.device = agent.device\n\n    def collect_rollout(self, num_steps=2048):\n        """Collect rollout data for training."""\n        obs_list = []\n        action_list = []\n        reward_list = []\n        done_list = []\n        log_prob_list = []\n\n        obs = self.env.reset()\n        obs = torch.FloatTensor(obs).to(self.device)\n\n        for step in range(num_steps):\n            with torch.no_grad():\n                action, log_prob = self.agent.actor_critic.get_action(obs)\n\n            next_obs, reward, terminated, truncated, info = self.env.step(action.cpu().numpy())\n\n            obs_list.append(obs.cpu().numpy())\n            action_list.append(action.cpu().numpy())\n            reward_list.append(reward)\n            done_list.append(terminated | truncated)\n            log_prob_list.append(log_prob.cpu().numpy())\n\n            obs = torch.FloatTensor(next_obs).to(self.device)\n\n        return (\n            np.array(obs_list),\n            np.array(action_list),\n            np.array(reward_list),\n            np.array(done_list),\n            np.array(log_prob_list)\n        )\n\n    def train(self, total_timesteps=1000000):\n        """Main training loop."""\n        timesteps = 0\n\n        while timesteps < total_timesteps:\n            # Collect rollout\n            obs, actions, rewards, dones, log_probs = self.collect_rollout()\n\n            # Update agent\n            metrics = self.agent.update(obs, actions, rewards, dones, log_probs)\n\n            timesteps += len(obs)\n\n            # Logging\n            if timesteps % 10000 == 0:\n                print(f"Timesteps: {timesteps}, "\n                      f"Policy Loss: {metrics[\'policy_loss\']:.4f}, "\n                      f"Value Loss: {metrics[\'value_loss\']:.4f}, "\n                      f"KL: {metrics[\'kl_div\']:.4f}")\n'})}),"\n",(0,r.jsx)(e.p,{children:"The PPO implementation for humanoid control includes several key features:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Continuous Action Spaces"}),": The Gaussian policy handles the continuous joint space of humanoid robots."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"GAE Computation"}),": Generalized Advantage Estimation provides more stable value estimates."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Clipped Objective"}),": The PPO objective prevents large policy updates that could destabilize learning."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Adaptive KL Control"}),": APPO adjusts the learning rate based on KL divergence to maintain stable learning."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:["OpenAI, et al. (2025). Learning Humanoid Locomotion with Proximal Policy Optimization. ",(0,r.jsx)(e.em,{children:"NeurIPS 2025"}),". ",(0,r.jsx)(e.a,{href:"https://doi.org/10.48665/neurips.2025.12347",children:"DOI:10.48665/neurips.2025.12347"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"curriculum-learning-for-humanoid-skills",children:"Curriculum Learning for Humanoid Skills"}),"\n",(0,r.jsx)(e.p,{children:"Curriculum learning in Isaac Lab for humanoid robots involves systematically increasing task difficulty to enable progressive skill acquisition. This approach is essential for complex behaviors like walking, running, and manipulation, where direct training on difficult tasks often fails. The curriculum starts with simplified environments and gradually introduces complexity, perturbations, and challenging conditions."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"class HumanoidCurriculum:\n    \"\"\"Curriculum learning framework for humanoid robots.\"\"\"\n\n    def __init__(self, env):\n        self.env = env\n        self.current_stage = 0\n        self.stage_progress = 0.0\n        self.performance_history = []\n\n        # Define curriculum stages\n        self.stages = [\n            {\n                'name': 'balance_training',\n                'difficulty': 1.0,\n                'episode_length': 250,\n                'perturbations': [],\n                'success_threshold': 0.7,\n                'next_stage_threshold': 0.85\n            },\n            {\n                'name': 'simple_locomotion',\n                'difficulty': 1.5,\n                'episode_length': 300,\n                'perturbations': ['small_push'],\n                'success_threshold': 0.6,\n                'next_stage_threshold': 0.8\n            },\n            {\n                'name': 'robust_locomotion',\n                'difficulty': 2.0,\n                'episode_length': 400,\n                'perturbations': ['push', 'uneven_terrain'],\n                'success_threshold': 0.5,\n                'next_stage_threshold': 0.75\n            },\n            {\n                'name': 'complex_manipulation',\n                'difficulty': 2.5,\n                'episode_length': 500,\n                'perturbations': ['push', 'uneven_terrain', 'object_variations'],\n                'success_threshold': 0.4,\n                'next_stage_threshold': 0.7\n            }\n        ]\n\n    def evaluate_performance(self, episode_rewards, episode_lengths):\n        \"\"\"Evaluate agent performance to determine curriculum progression.\"\"\"\n        if len(episode_rewards) < 10:\n            return 0.0  # Not enough data\n\n        # Calculate average performance over recent episodes\n        recent_rewards = episode_rewards[-10:]\n        avg_reward = np.mean(recent_rewards)\n\n        # Normalize by episode length to account for varying episode lengths\n        recent_lengths = episode_lengths[-10:]\n        avg_length = np.mean(recent_lengths)\n\n        # Calculate normalized performance\n        normalized_performance = avg_reward / avg_length\n\n        return normalized_performance\n\n    def update_curriculum(self, episode_rewards, episode_lengths):\n        \"\"\"Update curriculum based on performance.\"\"\"\n        current_performance = self.evaluate_performance(episode_rewards, episode_lengths)\n\n        # Store performance in history\n        self.performance_history.append(current_performance)\n\n        # Check if we can advance to next stage\n        current_stage_info = self.stages[self.current_stage]\n\n        if (current_performance > current_stage_info['next_stage_threshold'] and\n            len(self.performance_history) > 20 and  # Ensure stability\n            np.std(self.performance_history[-10:]) < 0.05):  # Performance is stable\n\n            if self.current_stage < len(self.stages) - 1:\n                self.current_stage += 1\n                print(f\"Advancing to curriculum stage: {self.stages[self.current_stage]['name']}\")\n                self.apply_stage_settings(self.current_stage)\n\n        # Apply current stage settings\n        self.apply_stage_settings(self.current_stage)\n\n    def apply_stage_settings(self, stage_idx):\n        \"\"\"Apply settings for current curriculum stage.\"\"\"\n        stage = self.stages[stage_idx]\n\n        # Update episode length\n        self.env.episode_length_buf[:] = stage['episode_length']\n\n        # Apply perturbations\n        self.apply_perturbations(stage['perturbations'])\n\n        # Adjust reward weights based on stage\n        self.adjust_reward_weights(stage_idx)\n\n    def apply_perturbations(self, perturbation_types):\n        \"\"\"Apply environmental perturbations.\"\"\"\n        for pert_type in perturbation_types:\n            if pert_type == 'small_push':\n                self.env.cfg.sim.add_force_noise = True\n                self.env.cfg.sim.force_noise_range = (-5.0, 5.0)\n            elif pert_type == 'push':\n                self.env.cfg.sim.add_force_noise = True\n                self.env.cfg.sim.force_noise_range = (-20.0, 20.0)\n            elif pert_type == 'uneven_terrain':\n                self.add_uneven_terrain()\n            elif pert_type == 'object_variations':\n                self.add_object_variations()\n\n    def add_uneven_terrain(self):\n        \"\"\"Add uneven terrain to environment.\"\"\"\n        # In Isaac Lab, this would involve modifying the terrain generator\n        # to create more challenging ground conditions\n        pass\n\n    def add_object_variations(self):\n        \"\"\"Add variations to objects in manipulation tasks.\"\"\"\n        # Vary object properties like mass, friction, and position\n        pass\n\n    def adjust_reward_weights(self, stage_idx):\n        \"\"\"Adjust reward function weights based on curriculum stage.\"\"\"\n        stage = self.stages[stage_idx]\n\n        # Different stages may emphasize different aspects\n        if stage['name'] == 'balance_training':\n            self.env.cfg.curriculum['reward_weights'] = {\n                'balance': 2.0,\n                'action_smoothness': 1.0,\n                'upright': 1.5,\n                'progress': 0.1\n            }\n        elif stage['name'] == 'simple_locomotion':\n            self.env.cfg.curriculum['reward_weights'] = {\n                'balance': 1.0,\n                'action_smoothness': 0.5,\n                'upright': 1.0,\n                'progress': 2.0,\n                'joint_regularization': 0.5\n            }\n        elif stage['name'] == 'robust_locomotion':\n            self.env.cfg.curriculum['reward_weights'] = {\n                'balance': 1.0,\n                'action_smoothness': 0.3,\n                'upright': 0.8,\n                'progress': 1.5,\n                'joint_regularization': 0.3,\n                'foot_placement': 1.0,\n                'energy_efficiency': 0.5\n            }\n        elif stage['name'] == 'complex_manipulation':\n            self.env.cfg.curriculum['reward_weights'] = {\n                'balance': 1.0,\n                'manipulation_success': 3.0,\n                'grasp_quality': 2.0,\n                'action_smoothness': 0.5,\n                'end_effector_control': 1.5\n            }\n\n    def get_curriculum_status(self):\n        \"\"\"Get current curriculum status.\"\"\"\n        return {\n            'current_stage': self.current_stage,\n            'stage_name': self.stages[self.current_stage]['name'],\n            'difficulty': self.stages[self.current_stage]['difficulty'],\n            'recent_performance': self.performance_history[-5:] if self.performance_history else []\n        }\n"})}),"\n",(0,r.jsx)(e.p,{children:"The curriculum learning approach for humanoid robots includes several important aspects:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Progressive Difficulty"}),": Tasks start simple and gradually increase in complexity."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Performance-Based Advancement"}),": Stage progression is based on actual performance metrics."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Environmental Perturbations"}),": Gradual introduction of disturbances to improve robustness."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Reward Function Adaptation"}),": Different stages emphasize different behaviors through reward shaping."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Stability Requirements"}),": Advancement requires both high performance and stability."]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"advanced-humanoid-control-strategies",children:"Advanced Humanoid Control Strategies"}),"\n",(0,r.jsx)(e.p,{children:"Advanced control strategies for humanoid robots in Isaac Lab combine multiple techniques to achieve robust and versatile behavior. These include hierarchical control architectures, model predictive control integration, and adaptive control methods that adjust to changing conditions and environments."}),"\n",(0,r.jsx)(e.p,{children:"The control hierarchy typically includes:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"High-Level Planning"}),": Path planning and task sequencing"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Trajectory Generation"}),": Reference trajectory generation for locomotion and manipulation"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Whole-Body Control"}),": Coordinated control of all degrees of freedom"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Low-Level Actuator Control"}),": Direct motor control and feedback"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"integration-with-real-hardware",children:"Integration with Real Hardware"}),"\n",(0,r.jsx)(e.p,{children:"The final step in Isaac Lab-based humanoid development is the integration with real hardware. This involves careful calibration of simulation parameters to match real robot dynamics, sensor characteristics, and environmental conditions. The transition from simulation to reality requires validation of learned policies and potential fine-tuning to account for model inaccuracies and environmental differences."})]})}function m(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(u,{...n})}):u(n)}}}]);